Machine learning is a branch of Computer Science It focuses on the use of data and algorithms to imitate the way humans learn 
Machine Learning is an important field of data science because there is too much data in the world for humans to process and Classical Machine Learning is dependent on human Intervention which is a sub-field of AI that uses algorithms trained on data to produce adaptable models to perform tasks  
A computer program is said to learn from experience with respect to some class of tasks and measure  the performance with experience Machine Learning is the field of study that gives the computer the ability to learn without being explicitly programmed. 
Deep learning is a subset of Machine learning which can work with or without human intervention, it learns using both supervised and unsupervised learning subset of machine learning that uses layers of Neural Networks to do the most complex ML tasks 
Unlike traditional ML, it does not require manual feature extraction and keeps getting better with more data Neural Networks but Deep learning requires large volume of high-dimensional data. Deep Learning models can learn from high-dimensional, unstructured data (images, texts, etc.) Deep learning excel in complex tasks requiring hierarchical feature learning 
Machine learning is used when the task is simple and structured enough for Machine Learning models, When computational resources are minimal and When model interpretation is required
Deep Learning is being used today for Image Recognition: Automating the process of identifying and detecting objects in images and videos Natural Language Processing: Understanding and generating human language, enabling applications such as chat bots and translation services. Deep Learning is used today in Healthcare: Assisting in diagnosis, personalized medicine, and drug discovery. Autonomous Vehicles: Enabling self-driving cars to navigate and understand their environment. Finance: Fraud detection, credit scoring, and algorithmic trading.           
ai is an umbrella term for software that mimics human cognition to perform complex tasks such as Speech Recognition, object detection, text generation and more, It is often conflated with deep learning.  
A lot of programs allow users to speak instead of typing with the help of AI This increases a program’s accessibility to the differently abled Also allows for a more natural interaction with computers Examples Google Assistant Speech-to-Text keyboards Customer Service The advent of ChatGPT and other chatbots have led to creation of customer service chatbots Computer Vision Everytime you take a picture AI is used to determine the best way to tune color exposure and lighting When the picture is taken AI is used to tag faces in the picture for various reasons Recommendation Engines Using past customer data AI models can help recommend content that customer likes to consume TikTok Youtube Instagram Facebook etc perfected the craft Advertisement platforms are built with the promise of connecting the right ads to the right user Fraud Detection Banks and other financial institutions use AI to spot suspicious transaction This is to protect their customers from malicious intent Email services also detect and delete fraudulent emails from entering your inbox     
Machine learning algorithms are used to either make a prediction or classify a given data input. The data may or may not be labeled. The way a machine learning algorithm learns is with the help of a mathematical function which is responsible to evaluate the prediction of a model with it’s true label Models are then adjusted to reduce discrepancy between a known example and the model estimate with the help of a loss function. 
there are mainly 3 types of ML Learning - supervised learning, unsupervised learning and reinforcement learning. There is also semi-supervised learning which is not the most common.
Supervised Learning This is the most popular method to train algorithms A labeled dataset is used to train algorithms The algorithm learns patterns from labeled data during training and predict outcomes for new, unseen data Can be used to learn classification or regression Trained models aim to generalize, by avoiding overfitting/underfitting
Unsupervised learning is when unlabeled data is used to cluster similar data together. System learns without direct human supervision Widely used in Clustering Anomaly detection Association mining Data preprocessing Example algorithms K-means PCA SVD ICA 
Semi-supervised learning is similar to clustering of unlabeled data which then requires the intervention of a human to label the data  
Reinforcement learning is when the model learns to take decisions in an environment where the decisions are then evaluated with positive or negative feedback to reinforce the correct behavior. 
Generalization: Model should generalize by learning the underlying patterns in the data, rather than memorizing the data exactly
The drawbacks of using an ML model is that there will might be over fitting or under fitting of data 
Overfitting Occurs when a model fits the training data too closely and can only train well but test poorly because the Model has memorized data 
Underfitting: Occurs when the model is too simple and has not captured the underlying pattern because the Model did not learn anything 
Example of models that are used during supervised learning Neural Networks Naive Bayes Random Forest Gradient Boosting 
Unsupervised Learning An unlabeled dataset is used to train algorithms Used often to uncover hidden patterns or data grouping without human intervention Ideal for Exploratory data analysis Image and Pattern Recognition Example of models that are used during unsupervised learning Neural Networks K-means clustering Principal Component Analysis (PCA) Singular Value Decomposition (SVD) Machine Learning Use DATA and ANSWERS to learn the underlying set of RULES by fine-tuning long list of rules Getting insights from large amounts of data  
Model based learning models are the type of supervised models that learn underlying patterns from the data and then generalizes on new data by extending the parameters that are learned from the training data e.g. Spam filter may learn on the fly with a deep neural network – online model-based supervised learning system 
Instance based learning is when the model learns from only the data that is provided without any further generalization. It updates its learning with the input of new data at every instance. 
Online Learning Can continue to learn after deployment Can take advantage of parallel computing – no down time Preferred choice in production
Online learning is the type of model learning where the model learns incrementally on the fly, it requires less computational power and information is updated to the model instantaneously. 
Batch Learning Not capable of learning after deployment Must be retrained from scratch computationally expensive
Batch learning is the process through which the model is trained with all possible training data before deployment. 
The learning system (agent) can Observe the environment Select and perform an action Get rewards/penalties as a result Learns what the best policy should be Policy defines what actions should be chosen in a certain situation Very effective in controlled environments (such as a game of chess) With the progress in deep learning increasingly used in more complex tasks (such as driving the mars rover).
Insufficient quantity Non-representative data Poor-quality data Overfitting data Underfitting data Most common problem in ML do not overgeneralize.
Training data fed to algorithm includes the desired answers/solutions (labels) Example algorithms Linear Regression Logistic Regression SVM Decision Tree Neural Network 
Constrain model to keep it simple – reduce risk of overfitting Hyperparameters – control level of regularization Get more training data, and reduce noise in it 
A good or bad model is identified with the help of model evaluation. Once the training of the model is complete it is then tested on new data data not seen by the model ever before Keep 80% for training, set 20% for testing NEVER go below 10% test data better model is better than better “accuracy” in order to regularize Keep a portion of training data held out for validation Alternatively, use cross-validation Pick the hyperparameters that work best on validation for your model on the test dataset. A great model is trained with 60% training data, 20% validation data, and 20% testing data an okay model trained with 70% training data, 15% validation data, and 15% testing data a barely acceptable model trained with 80% training data, 10% validation data, and 10% testing data Only way to know for sure which model works best is to evaluate them Make reasonable assumptions about your data to select model. 
To avoid overfitting/underfitting To ensure a model’s prediction are reliable To evaluate how a model may work in real-world scenarios Aid engineers to make decisions about model deployment 
Any evaluation we make has to be an objective one There are several metrics to evaluate a model, for example accuracy precision recall specificity and f1-score these are used in the case of a classification problem.  
Overfitting is when model memorizes training data This is the opposite of learning a pattern and generalizing We can identify if our model is overfitted if: There is high performance on training data There is poor performance on test data 
Underfitting happens when the model is too simple The underlying data patterns have not been captured We can identify if our model has underfitting if Low performance on training data Low performance on test data
A perfect model fit can be achieved By first constructing good evaluation metrics to give us feedback on model performance Then tuning hyper-parameters till the performance improves across the board 
Accuracy is calculated by dividing the total correct predictions with the total number of predictions Ideal Usage When class distributions are balanced 
Precision counts the true positives out of all the items predicted to be positive Ideal Usage When the cost of false positive is too high Example: Email spam detection
Recall counts how many of the true positive items were correctly classified Ideal Usage When the missing a positive is too costly 
The average of precision and recall Ideal Usage When a balance between precision and recall is required.
Mean absolute error (MAE) is The average of the absolute differences between the predicted values and actual values Ideal Usage: When you want to understand the magnitude of error without regard to direction, especially in contexts where large errors aren't more significant than small ones
Mean Squared Error (MSE) The average of the squared differences between the predicted values and actual values Ideal Usage: When larger errors are particularly undesirable and should be penalized more. Such as stock market predictions 
Root Mean Squared Error (RMSE) The square root of MSE, offering error magnitude in the same units as the predicted values Ideal Usage When you want to interpret the error in the original unit and penalize larger errors. Such as predicting the price of houses 
R-squared (Coefficient of Determination) Represents the proportion of variance for the dependent variable that's explained by independent variables Ideal Usage: When you want to understand the proportion of the dataset's variability captured by the model. Such as how much variance in exam scores are explained by hours studied 
 Adjusted R-squared Modifies R-squared to account for the number of predictors in the model, penalizing excessive use of features. 
Mean Bias deviation (MBD) The average difference between the predicted and actual values, indicating the direction of the error Ideal Usage: When you're interested in the direction of the error (overestimation vs. underestimation). Helps vary it over or under the limits set                     
Statistical Machine Learning emphasizes on the statistical properties of datasets This is most commonly used where predictions are paramount Stock Market forecasting Medical Diagnosis statistical models play the role of the function that is “fitted” onto the Dataset The model takes in data X and predicts an output Y We evaluate the model with loss functions Lets learn a few of those models today     
Linear Regression is one of the simplest and most widely used statistical technique It’s goal is to model a relationship between a single dependent variable with one or more multiple independent variable. 
We use linear regression when the relationship between the independent and dependent variable is believed to be linear. Continuous Output: When predicting values that are continuous (e.g house prices temperatures). Interpretability: When it's important to understand the influence of each feature on the output. Linear regression provides coefficients for each feature which indicate their relative importance. Linear Regression works best when there is a linear relationship between the predictors and the response Regression tasks predict a value based on input data Works best when Residuals are normally distributed Residuals have constant variance Cost Function in Linear Regression Mean Square Error (MSE): Average squared difference between actual and predicted values Adjust model parameters to minimize MSE.      
Logistic Regression predicts the probability of occurrence of an event by fitting data to a logistic curve 
When to use Logistic Regression Binary Outcome: When the dependent variable is binary (e.g., spam or not spam, churn or not churn) Probabilistic Results: When you need to know the probability of your output. Logistic regression doesn’t just give a binary outcome it gives the probability of that outcome Feature Importance: Similar to linear regression, logistic regression provides coefficients that can help in understanding the influence of features. Cost Function in Logistic Regression Log-Loss: Measure the performance of a classification model whose output probability value is between 0 and 1  
Decision Trees split data into subsets This process is repeated recursively Results in a tree-like model of decisions Components of Decision Trees Root Node: Represents the entire dataset, gets divided Decision Node: When a sub-node splits into further sub-nodes Leaf Node: Nodes that contain the decision to be taken The splitting criteria for the decision tree are Impurity: Measures how often a randomly chosen element would be incorrectly classified Entropy: Measures randomness or unpredictability in the dataset Information Gain: The entropy of the original dataset minus the weighted average entropy of the split datasets. Issues in decision tree can include Overfitting: When a model captures noise in the training data and performs poorly on new, unseen data. Complex Trees: Trees that are too deep can capture noise. Pruning: Process of reducing the size of a tree by turning some branch nodes into leaf nodes to reduce complexity. When to use decision trees Non-linear Relationships: Decision trees can capture nonlinear relationships between features and the target variable Interpretability: They are easy to visualize and understand, making them great for deriving insights and rules Categorical Input Features: They handle categorical variables easily Feature Interactions: Decision trees can inherently capture interactions between features  
Random Forests are an ensemble of decision trees Each tree in a Random Forest is trained on a random subset of data Bagging: A feature of Random Forests that aggregates decision of individual trees and reduces variance. Advantages of using random forest includes Reduction in Overfitting: Diversity among trees reduces chances of overfitting. Feature Importance: Ability to rank features based on their importance in making predictions. Handling Missing Values: Can handle missing data without explicit imputation Generalization: Often generalizes better to new data than individual trees. Disadvantages of random forests Interpretability: Harder to interpret than a single decision tree. Computation: Requires more computational resources. Forest Size: Need to choose the number of trees (more isn't always better). When to use random forests High Accuracy: When performance is a primary concern. Random forests generally yield better accuracy than individual decision trees Feature Importance: Random forests can rank features based on their importance in making accurate predictions. Handling Overfitting: Random Forests, through bagging, tend to reduce the overfitting that can be observed with individual decision trees. Handling Large Data: They can handle datasets with a higher dimensionality and can manage missing values. Non-linear Data: They can capture non-linear feature interactions.     
support vector machine is a supervised ML algorithm which can be used for both classification or regression It performs classification by finding the hyperplane that best divides dataset into classes Hyperplane is a generalized plane in different dimensions. When support vector machines are used for text classification problems when there is a need for margin separation for complex datasets where linear separation is not obvious the drawbacks of support vector machine includes it is inefficient on large datasets it is sensitive to noise and requires fine tuning using parameters such as the kernels support vector machines should be avoided when there is a large dataset when the dataset has a lot of noise when there is no clear margin or separation  
K nearest neighbors is a non-parametric, lazy learning algorithm It assumes the similarity between the new data input with the available data Then assigns the new data into the category that is most similar to the available data categories when to use k nearest neighbors When the dataset is relatively small The data has little noise The data has decision boundaries which are very irregular the drawbacks of using k nearest neighbors is that KNN becomes significantly slower as the number of examples grows It is sensitive to irrelevant or redundant features as all features contribute to the similarity. KNN should be avoided when there is a relatively large dataset when the data has a high number of dimension and when the dataset has a lot of noise.        
Gradient Boosting is a boosting algorithm that combines several weak learners into strong learners It is an ensemble method the Initialization Begins with a simple model It builds a sequence of trees where each new tree corrects the errors of its predecessors these are called residuals. Gradient boosting is used when there is an unbalanced dataset or when the model performance is the primary concern. Drawbacks of such a model is that it can overfit on noisy data requires careful tuning of parameters and Longer training time as trees are built sequentially when to avoid K nearest neighbors when time is a constraint or when the task is too simple and a simple model will suffice.          
Neural Networks are the backbone of Deep Learning These are mathematical model based on human brain structure At a high level these consists of Nodes/Neurons which host a value Connections from one node to another Basic Components of a Neural Network Neurons: Fundamental units of a neural network that receive input and pass the output to the next layer after computation. Weights: Parameters within the network that transform input data within the network's layers. Biases: Additional parameters that enable the model to adjust its output accordingly. Activation Functions: Determine if a neuron should be activated or not, influencing the model's output.   
Classification involves predicting discrete classes, Classification often deals with skewed datasets Accuracy is not the preferred performance measure for classification. There are other measures for classification problems such as precision, specificity, recall and f1-score all of this can be calculates with a confusion matrix. There is always a trade off between precision and recall. The higher the precision of the model, the lower the recall rate of the model. The receiver operating Characteristics is a curve that plots the true positive rate against the false positive rate. A point closest to the top left corner of the plot is the best choice for the model. A perfect classifier will have an area under the curve (AOC) of a receiver operating characteristics(ROC) as 1. A purely random classifier will have the AUC as 0.5    
NLP is a branch of artificial intelligence that deals with the interaction between computers and humans through the natural languages 
The objective of NLP is to read, decipher, understand, and make sense of human languages in a valuable way. This is achieved as NLP combines computational linguistics—rule-based modeling of human language—with statistical, machine learning, and deep learning models
There are many instances where NLP is utilized to provide value to both companies and individuals. Some of these examples are Classification: Sentiment Analysis, Token classification, gender text alignment Sequence Generation: ASR, QA, Fill-Mask, NSP, Translation and Multiple Choice: Choosing between several candidates. 
Sentiment Analysis is the automated process of identifying and categorizing opinions expressed in text to determine the writer's attitude towards a particular topic or product Example: A company uses sentiment analysis to monitor social media mentions of their brand, quickly identifying and addressing customer complaints, and leveraging positive feedback for marketing campaigns
Token Classification is the process of identifying types of tokens present in a text, this refers to particular buzz word that might be of importance in the sentence Token Classification might be introduced in a e-commerce site to identify important feedback in a comment. 
The task of Question Answering can answer a question given a context and sometimes without context Example: QA chatbots or search engine models like ChatGPT and BingAI to answer general closed domain questions
An ASR system recognizes and processes audio to generate transcriptions relevant to the audio the input includes an audio sequence and the output is a text sequence. 
NLP relies on the understanding of linguistic principles to accurately interpret and generate human language  Grasping these fundamentals is crucial for creating sophisticated NLP models that can accurately mimic human understanding and production of language  
NLP models need to pay attention to the syntax, relevance, and the semantics of the sentence. Syntax refers to the structure of the sentences whilst semantics refer to the cohesion of the phrases and understanding the meaning with respect to the context in which it is said. 
Morphology examines the structure of words and how they are formed from smaller units called morphemes (the smallest grammatical unit in a language) Morphological analysis is used in NLP for stemming (reducing words to their base form) and lemmatization (finding the lemma of a word based on its intended meaning)
Text data is unstructured and often noisy. Special preprocessing is required in NLP to Remove irrelevant characters and words that could mislead the analysis  Reduce complexity to improve computational efficiency. Enhance the model's ability to generalize from the training data Address the intricacies and nuances of human language 
Another stage of preprocessing in NLP is stopword and punctuation removal. Stopwords are commonly used words (such as 'the', 'is', 'at') that are filtered out before processing since they add noise without informative content this helps with focused analysis and faster processing Caution is advised as some stopwords can change the meaning of a sentence (e.g., 'not'). Punctuation marks are often removed during text preprocessing because: They can be irrelevant for understanding the meaning of texts, especially in models focusing on individual words. However, in certain contexts like sentiment analysis, exclamation points or question marks can carry sentiment and should be preserved.
Once the stopwords and punctuations are removed from the sentences the next stage in NLP preprocessing is normalization and lemmatization of the text Normalization standardizes text, such as converting to lowercase, while lemmatization reduces words to their base or dictionary form this Helps in reducing the number of unique tokens in the text  Lemmatization takes into account the morphological analysis of the words, aiming to remove inflectional endings only and to return the base or dictionary form of a word 
After lemmatization and stemming the final part of the preprocessing includes parts of speech tagging and tokenization Part of Speech (POS) tagging assigns word types to each word (noun, verb, adjective, etc.) Essential for understanding the structure of sentences Helps in disambiguating words that can represent more than one part of speech (e.g., 'can' as a verb or a noun). 
Tokenization is the process of breaking text into individual terms or tokens. Can be as simple as splitting by space, or as complex as recognizing words in a sentence.   
Feature Engineering is the process of converting raw data into a numerical format that algorithms can utilize for prediction or classification Feature engineering in NLP is crucial for transforming text into  a structured, machine-readable form there are many ways in which this can be achieved some of the methods include Bag of words – countvectorizer, term frequency inverse document frequency TF-IDF, GloVe(Global vectors for word representation), these are the statistical ways in which texts are converted into numerical version of itself. This process can also be achieved with the help of neural networks like Word2Vec embedding, SentenceBERT.   
BoW: Counts the occurrence of each word in a document, ransforming text into a numerical vector. Each word becomes a feature.This is a simple approach and is a good starting point to convert texts into numbers. However, it ignores the grammar and word order and treats all the words in the sentence with equal importance. 
TF-IDF addresses one of BoW's key limitations by considering not just frequency but the importance of words within a document set TF-IDF reduces the weight of common words like 'the' or 'is'  across documents, which are less informative, and increases the weight for words that are unique to a specific document his method allows us to surface more relevant terms in our analysis and to better distinguish between documents based on their unique content  Despite its sophistication over BoW, TF-IDF still does not  account for the semantics of word order or context    
Word2Vec represents words by their context, capturing semantic relationships in a dense vector space Through neural networks, Word2Vec predicts a word from its neighbors, or vice versa, learning vectors that place semantically similar words close together This model goes beyond frequency, allowing algorithms to understand similarity and analogy based on word usage patterns Though powerful, Word2Vec requires significant data and computational power, and it does not inherently capture the meaning of larger text structures like sentences or paragraphs 
 SentenceBert adapts the powerful BERT model to generate embeddings that represent the meaning of entire sentences, not just words By using siamese and triplet network structures, SentenceBert is trained to understand the nuanced differences and similarities between sentences these embeddings excel in tasks requiring deep semantic understanding, such as semantic text similarity, clustering, and information retrieval The trade-off for this depth of understanding is the need for greater computational resources and more complex model fine-tuning     
GloVe builds a co-occurrence matrix that records how often each word appears in the context of every other word It combines the advantages of matrix factorization methods with the contextual benefits of Word2Vec, offering a rich, nuanced view of word meanings 
Named entity recognition is an NLP task that identifies and classifies key information (entities) in text into predefined categories. Entity Types: Common categories include names of people, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.
A language model predicts the likelihood of a sequence of words. It's a statistical tool that helps computers understand human language 
Types of language models include: Statistical models, Rule based models, Neural Network based models Typical use cases include Language Translation, Text generation, Speech predictions, etc. 
