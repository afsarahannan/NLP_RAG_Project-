Machine learning is a branch of Computer Science which focuses on the use of data and algorithms to imitate the way humans learn 
Machine Learning is an important field of data science because there is too much data in the world for humans to process and Classical Machine Learning is dependent on human Intervention which is a sub-field of AI that uses algorithms trained on data to produce adaptable models to perform tasks  
A computer program is said to learn from experience with respect to some class of tasks and measure  the performance with experience Machine Learning is the field of study that gives the computer the ability to learn without being explicitly programmed. 
Deep learning is a subset of Machine learning which can work with or without human intervention, it learns using both supervised and unsupervised learning subset of machine learning that uses layers of Neural Networks to do the most complex ML tasks 
Unlike traditional ML, it does not require manual feature extraction and keeps getting better with more data Neural Networks but Deep learning requires large volume of high-dimensional data. Deep Learning models can learn from high-dimensional, unstructured data (images, texts, etc.) Deep learning excel in complex tasks requiring hierarchical feature learning 
Machine learning is used when the task is simple and structured enough for Machine Learning models, When computational resources are minimal and When model interpretation is required
Deep Learning is being used today for Image Recognition: Automating the process of identifying and detecting objects in images and videos Natural Language Processing: Understanding and generating human language, enabling applications such as chat bots and translation services. Deep Learning is used today in Healthcare: Assisting in diagnosis, personalized medicine, and drug discovery. Autonomous Vehicles: Enabling self-driving cars to navigate and understand their environment. Finance: Fraud detection, credit scoring, and algorithmic trading.           
ai is an umbrella term for software that mimics human cognition to perform complex tasks such as Speech Recognition, object detection, text generation and more, It is often conflated with deep learning.  
A lot of programs allow users to speak instead of typing with the help of AI This increases a program’s accessibility to the differently abled Also allows for a more natural interaction with computers Examples Google Assistant Speech-to-Text keyboards Customer Service The advent of ChatGPT and other chatbots have led to creation of customer service chatbots Computer Vision Everytime you take a picture AI is used to determine the best way to tune color exposure and lighting When the picture is taken AI is used to tag faces in the picture for various reasons Recommendation Engines Using past customer data AI models can help recommend content that customer likes to consume TikTok Youtube Instagram Facebook etc perfected the craft Advertisement platforms are built with the promise of connecting the right ads to the right user Fraud Detection Banks and other financial institutions use AI to spot suspicious transaction This is to protect their customers from malicious intent Email services also detect and delete fraudulent emails from entering your inbox     
Machine learning algorithms are used to either make a prediction or classify a given data input. The data may or may not be labeled. The way a machine learning algorithm learns is with the help of a mathematical function which is responsible to evaluate the prediction of a model with it’s true label Models are then adjusted to reduce discrepancy between a known example and the model estimate with the help of a loss function. 
there are mainly 3 types of ML Learning - supervised learning, unsupervised learning and reinforcement learning. There is also semi-supervised learning which is not the most common.
Supervised Learning This is the most popular method to train algorithms A labeled dataset is used to train algorithms The algorithm learns patterns from labeled data during training and predict outcomes for new, unseen data Can be used to learn classification or regression Trained models aim to generalize, by avoiding overfitting/underfitting
Unsupervised learning is when unlabeled data is used to cluster similar data together. System learns without direct human supervision Widely used in Clustering Anomaly detection Association mining Data preprocessing Example algorithms K-means PCA SVD ICA 
Semi-supervised learning is similar to clustering of unlabeled data which then requires the intervention of a human to label the data  
Reinforcement learning is when the model learns to take decisions in an environment where the decisions are then evaluated with positive or negative feedback to reinforce the correct behavior. 
Generalization: Model should generalize by learning the underlying patterns in the data, rather than memorizing the data exactly
The drawbacks of using an ML model is that there will might be over fitting or under fitting of data 
Overfitting Occurs when a model fits the training data too closely and can only train well but test poorly because the Model has memorized data 
Underfitting: Occurs when the model is too simple and has not captured the underlying pattern because the Model did not learn anything 
Example of models that are used during supervised learning Neural Networks Naive Bayes Random Forest Gradient Boosting 
Unsupervised Learning An unlabeled dataset is used to train algorithms Used often to uncover hidden patterns or data grouping without human intervention Ideal for Exploratory data analysis Image and Pattern Recognition Example of models that are used during unsupervised learning Neural Networks K-means clustering Principal Component Analysis (PCA) Singular Value Decomposition (SVD) Machine Learning Use DATA and ANSWERS to learn the underlying set of RULES by fine-tuning long list of rules Getting insights from large amounts of data  
Model based learning models are the type of supervised models that learn underlying patterns from the data and then generalizes on new data by extending the parameters that are learned from the training data e.g. Spam filter may learn on the fly with a deep neural network – online model-based supervised learning system 
Instance based learning is when the model learns from only the data that is provided without any further generalization. It updates its learning with the input of new data at every instance. 
Online Learning Can continue to learn after deployment Can take advantage of parallel computing – no down time Preferred choice in production
Online learning is the type of model learning where the model learns incrementally on the fly, it requires less computational power and information is updated to the model instantaneously. 
Batch Learning Not capable of learning after deployment Must be retrained from scratch computationally expensive
Batch learning is the process through which the model is trained with all possible training data before deployment. 
The learning system (agent) can Observe the environment Select and perform an action Get rewards/penalties as a result Learns what the best policy should be Policy defines what actions should be chosen in a certain situation Very effective in controlled environments (such as a game of chess) With the progress in deep learning increasingly used in more complex tasks (such as driving the mars rover).
Insufficient quantity Non-representative data Poor-quality data Overfitting data Underfitting data Most common problem in ML do not overgeneralize.
Training data fed to algorithm includes the desired answers/solutions (labels) Example algorithms Linear Regression Logistic Regression SVM Decision Tree Neural Network 
Constrain model to keep it simple – reduce risk of overfitting Hyperparameters – control level of regularization Get more training data, and reduce noise in it 
A good or bad model is identified with the help of model evaluation. Once the training of the model is complete it is then tested on new data data not seen by the model ever before Keep 80% for training, set 20% for testing NEVER go below 10% test data better model is better than better “accuracy” in order to regularize Keep a portion of training data held out for validation Alternatively, use cross-validation Pick the hyperparameters that work best on validation for your model on the test dataset. A great model is trained with 60% training data, 20% validation data, and 20% testing data an okay model trained with 70% training data, 15% validation data, and 15% testing data a barely acceptable model trained with 80% training data, 10% validation data, and 10% testing data Only way to know for sure which model works best is to evaluate them Make reasonable assumptions about your data to select model. 
To avoid overfitting/underfitting To ensure a model’s prediction are reliable To evaluate how a model may work in real-world scenarios Aid engineers to make decisions about model deployment 
Any evaluation we make has to be an objective one There are several metrics to evaluate a model, for example accuracy precision recall specificity and f1-score these are used in the case of a classification problem.  
Overfitting is when model memorizes training data This is the opposite of learning a pattern and generalizing We can identify if our model is overfitted if: There is high performance on training data There is poor performance on test data 
Underfitting happens when the model is too simple The underlying data patterns have not been captured We can identify if our model has underfitting if Low performance on training data Low performance on test data
A perfect model fit can be achieved By first constructing good evaluation metrics to give us feedback on model performance Then tuning hyper-parameters till the performance improves across the board 
Accuracy is calculated by dividing the total correct predictions with the total number of predictions Ideal Usage When class distributions are balanced 
Precision counts the true positives out of all the items predicted to be positive Ideal Usage When the cost of false positive is too high Example: Email spam detection
Recall counts how many of the true positive items were correctly classified Ideal Usage When the missing a positive is too costly 
The average of precision and recall Ideal Usage When a balance between precision and recall is required.
Mean absolute error (MAE) is The average of the absolute differences between the predicted values and actual values Ideal Usage: When you want to understand the magnitude of error without regard to direction, especially in contexts where large errors aren't more significant than small ones
Mean Squared Error (MSE) The average of the squared differences between the predicted values and actual values Ideal Usage: When larger errors are particularly undesirable and should be penalized more. Such as stock market predictions 
Root Mean Squared Error (RMSE) The square root of MSE, offering error magnitude in the same units as the predicted values Ideal Usage When you want to interpret the error in the original unit and penalize larger errors. Such as predicting the price of houses 
R-squared (Coefficient of Determination) Represents the proportion of variance for the dependent variable that's explained by independent variables Ideal Usage: When you want to understand the proportion of the dataset's variability captured by the model. Such as how much variance in exam scores are explained by hours studied 
 Adjusted R-squared Modifies R-squared to account for the number of predictors in the model, penalizing excessive use of features. 
Mean Bias deviation (MBD) The average difference between the predicted and actual values, indicating the direction of the error Ideal Usage: When you're interested in the direction of the error (overestimation vs. underestimation). Helps vary it over or under the limits set                     
Statistical Machine Learning emphasizes on the statistical properties of datasets This is most commonly used where predictions are paramount Stock Market forecasting Medical Diagnosis statistical models play the role of the function that is “fitted” onto the Dataset The model takes in data X and predicts an output Y We evaluate the model with loss functions Lets learn a few of those models today     
Linear Regression is one of the simplest and most widely used statistical technique It’s goal is to model a relationship between a single dependent variable with one or more multiple independent variable. 
We use linear regression when the relationship between the independent and dependent variable is believed to be linear. Continuous Output: When predicting values that are continuous (e.g house prices temperatures). Interpretability: When it's important to understand the influence of each feature on the output. Linear regression provides coefficients for each feature which indicate their relative importance. Linear Regression works best when there is a linear relationship between the predictors and the response Regression tasks predict a value based on input data Works best when Residuals are normally distributed Residuals have constant variance Cost Function in Linear Regression Mean Square Error (MSE): Average squared difference between actual and predicted values Adjust model parameters to minimize MSE.      
Logistic Regression predicts the probability of occurrence of an event by fitting data to a logistic curve 
When to use Logistic Regression Binary Outcome: When the dependent variable is binary (e.g., spam or not spam, churn or not churn) Probabilistic Results: When you need to know the probability of your output. Logistic regression doesn’t just give a binary outcome it gives the probability of that outcome Feature Importance: Similar to linear regression, logistic regression provides coefficients that can help in understanding the influence of features. Cost Function in Logistic Regression Log-Loss: Measure the performance of a classification model whose output probability value is between 0 and 1  
Decision Trees split data into subsets This process is repeated recursively Results in a tree-like model of decisions Components of Decision Trees Root Node: Represents the entire dataset, gets divided Decision Node: When a sub-node splits into further sub-nodes Leaf Node: Nodes that contain the decision to be taken The splitting criteria for the decision tree are Impurity: Measures how often a randomly chosen element would be incorrectly classified Entropy: Measures randomness or unpredictability in the dataset Information Gain: The entropy of the original dataset minus the weighted average entropy of the split datasets. Issues in decision tree can include Overfitting: When a model captures noise in the training data and performs poorly on new, unseen data. Complex Trees: Trees that are too deep can capture noise. Pruning: Process of reducing the size of a tree by turning some branch nodes into leaf nodes to reduce complexity. When to use decision trees Non-linear Relationships: Decision trees can capture nonlinear relationships between features and the target variable Interpretability: They are easy to visualize and understand, making them great for deriving insights and rules Categorical Input Features: They handle categorical variables easily Feature Interactions: Decision trees can inherently capture interactions between features  
Random Forests are an ensemble of decision trees Each tree in a Random Forest is trained on a random subset of data Bagging: A feature of Random Forests that aggregates decision of individual trees and reduces variance. Advantages of using random forest includes Reduction in Overfitting: Diversity among trees reduces chances of overfitting. Feature Importance: Ability to rank features based on their importance in making predictions. Handling Missing Values: Can handle missing data without explicit imputation Generalization: Often generalizes better to new data than individual trees. Disadvantages of random forests Interpretability: Harder to interpret than a single decision tree. Computation: Requires more computational resources. Forest Size: Need to choose the number of trees (more isn't always better). When to use random forests High Accuracy: When performance is a primary concern. Random forests generally yield better accuracy than individual decision trees Feature Importance: Random forests can rank features based on their importance in making accurate predictions. Handling Overfitting: Random Forests, through bagging, tend to reduce the overfitting that can be observed with individual decision trees. Handling Large Data: They can handle datasets with a higher dimensionality and can manage missing values. Non-linear Data: They can capture non-linear feature interactions.     
support vector machine is a supervised ML algorithm which can be used for both classification or regression It performs classification by finding the hyperplane that best divides dataset into classes Hyperplane is a generalized plane in different dimensions. When support vector machines are used for text classification problems when there is a need for margin separation for complex datasets where linear separation is not obvious the drawbacks of support vector machine includes it is inefficient on large datasets it is sensitive to noise and requires fine tuning using parameters such as the kernels support vector machines should be avoided when there is a large dataset when the dataset has a lot of noise when there is no clear margin or separation  
K nearest neighbors is a non-parametric, lazy learning algorithm It assumes the similarity between the new data input with the available data Then assigns the new data into the category that is most similar to the available data categories when to use k nearest neighbors When the dataset is relatively small The data has little noise The data has decision boundaries which are very irregular the drawbacks of using k nearest neighbors is that KNN becomes significantly slower as the number of examples grows It is sensitive to irrelevant or redundant features as all features contribute to the similarity. KNN should be avoided when there is a relatively large dataset when the data has a high number of dimension and when the dataset has a lot of noise.        
Gradient Boosting is a boosting algorithm that combines several weak learners into strong learners It is an ensemble method the Initialization Begins with a simple model It builds a sequence of trees where each new tree corrects the errors of its predecessors these are called residuals. Gradient boosting is used when there is an unbalanced dataset or when the model performance is the primary concern. Drawbacks of such a model is that it can overfit on noisy data requires careful tuning of parameters and Longer training time as trees are built sequentially when to avoid K nearest neighbors when time is a constraint or when the task is too simple and a simple model will suffice.          
Neural Networks are the backbone of Deep Learning These are mathematical model based on human brain structure At a high level these consists of Nodes/Neurons which host a value Connections from one node to another Basic Components of a Neural Network Neurons: Fundamental units of a neural network that receive input and pass the output to the next layer after computation. Weights: Parameters within the network that transform input data within the network's layers. Biases: Additional parameters that enable the model to adjust its output accordingly. Activation Functions: Determine if a neuron should be activated or not, influencing the model's output.   
Classification involves predicting discrete classes, Classification often deals with skewed datasets Accuracy is not the preferred performance measure for classification. There are other measures for classification problems such as precision, specificity, recall and f1-score all of this can be calculates with a confusion matrix. There is always a trade off between precision and recall. The higher the precision of the model, the lower the recall rate of the model. The receiver operating Characteristics is a curve that plots the true positive rate against the false positive rate. A point closest to the top left corner of the plot is the best choice for the model. A perfect classifier will have an area under the curve (AOC) of a receiver operating characteristics(ROC) as 1. A purely random classifier will have the AUC as 0.5    
NLP is a branch of artificial intelligence that deals with the interaction between computers and humans through the natural languages 
The objective of NLP is to read, decipher, understand, and make sense of human languages in a valuable way. This is achieved as NLP combines computational linguistics—rule-based modeling of human language—with statistical, machine learning, and deep learning models
There are many instances where NLP is utilized to provide value to both companies and individuals. Some of these examples are Classification: Sentiment Analysis, Token classification, gender text alignment Sequence Generation: ASR, QA, Fill-Mask, NSP, Translation and Multiple Choice: Choosing between several candidates. 
Sentiment Analysis is the automated process of identifying and categorizing opinions expressed in text to determine the writer's attitude towards a particular topic or product Example: A company uses sentiment analysis to monitor social media mentions of their brand, quickly identifying and addressing customer complaints, and leveraging positive feedback for marketing campaigns
Token Classification is the process of identifying types of tokens present in a text, this refers to particular buzz word that might be of importance in the sentence Token Classification might be introduced in a e-commerce site to identify important feedback in a comment. 
The task of Question Answering can answer a question given a context and sometimes without context Example: QA chatbots or search engine models like ChatGPT and BingAI to answer general closed domain questions
An ASR system recognizes and processes audio to generate transcriptions relevant to the audio the input includes an audio sequence and the output is a text sequence. 
NLP relies on the understanding of linguistic principles to accurately interpret and generate human language  Grasping these fundamentals is crucial for creating sophisticated NLP models that can accurately mimic human understanding and production of language  
NLP models need to pay attention to the syntax, relevance, and the semantics of the sentence. Syntax refers to the structure of the sentences whilst semantics refer to the cohesion of the phrases and understanding the meaning with respect to the context in which it is said. 
Morphology examines the structure of words and how they are formed from smaller units called morphemes (the smallest grammatical unit in a language) Morphological analysis is used in NLP for stemming (reducing words to their base form) and lemmatization (finding the lemma of a word based on its intended meaning)
Text data is unstructured and often noisy. Special preprocessing is required in NLP to Remove irrelevant characters and words that could mislead the analysis  Reduce complexity to improve computational efficiency. Enhance the model's ability to generalize from the training data Address the intricacies and nuances of human language 
Another stage of preprocessing in NLP is stopword and punctuation removal. Stopwords are commonly used words (such as 'the', 'is', 'at') that are filtered out before processing since they add noise without informative content this helps with focused analysis and faster processing Caution is advised as some stopwords can change the meaning of a sentence (e.g., 'not'). Punctuation marks are often removed during text preprocessing because: They can be irrelevant for understanding the meaning of texts, especially in models focusing on individual words. However, in certain contexts like sentiment analysis, exclamation points or question marks can carry sentiment and should be preserved.
Once the stopwords and punctuations are removed from the sentences the next stage in NLP preprocessing is normalization and lemmatization of the text Normalization standardizes text, such as converting to lowercase, while lemmatization reduces words to their base or dictionary form this Helps in reducing the number of unique tokens in the text  Lemmatization takes into account the morphological analysis of the words, aiming to remove inflectional endings only and to return the base or dictionary form of a word 
After lemmatization and stemming the final part of the preprocessing includes parts of speech tagging and tokenization Part of Speech (POS) tagging assigns word types to each word (noun, verb, adjective, etc.) Essential for understanding the structure of sentences Helps in disambiguating words that can represent more than one part of speech (e.g., 'can' as a verb or a noun). 
Tokenization is the process of breaking text into individual terms or tokens. Can be as simple as splitting by space, or as complex as recognizing words in a sentence.   
Feature Engineering is the process of converting raw data into a numerical format that algorithms can utilize for prediction or classification Feature engineering in NLP is crucial for transforming text into  a structured, machine-readable form there are many ways in which this can be achieved some of the methods include Bag of words – countvectorizer, term frequency inverse document frequency TF-IDF, GloVe(Global vectors for word representation), these are the statistical ways in which texts are converted into numerical version of itself. This process can also be achieved with the help of neural networks like Word2Vec embedding, SentenceBERT.   
BoW: Counts the occurrence of each word in a document, transforming text into a numerical vector. Each word becomes a feature.This is a simple approach and is a good starting point to convert texts into numbers. However, it ignores the grammar and word order and treats all the words in the sentence with equal importance. 
TF-IDF addresses one of BoW's key limitations by considering not just frequency but the importance of words within a document set TF-IDF reduces the weight of common words like 'the' or 'is'  across documents, which are less informative, and increases the weight for words that are unique to a specific document his method allows us to surface more relevant terms in our analysis and to better distinguish between documents based on their unique content  Despite its sophistication over BoW, TF-IDF still does not  account for the semantics of word order or context    
Word2Vec represents words by their context, capturing semantic relationships in a dense vector space Through neural networks, Word2Vec predicts a word from its neighbors, or vice versa, learning vectors that place semantically similar words close together This model goes beyond frequency, allowing algorithms to understand similarity and analogy based on word usage patterns Though powerful, Word2Vec requires significant data and computational power, and it does not inherently capture the meaning of larger text structures like sentences or paragraphs 
 SentenceBert adapts the powerful BERT model to generate embeddings that represent the meaning of entire sentences, not just words By using siamese and triplet network structures, SentenceBert is trained to understand the nuanced differences and similarities between sentences these embeddings excel in tasks requiring deep semantic understanding, such as semantic text similarity, clustering, and information retrieval The trade-off for this depth of understanding is the need for greater computational resources and more complex model fine-tuning     
GloVe builds a co-occurrence matrix that records how often each word appears in the context of every other word It combines the advantages of matrix factorization methods with the contextual benefits of Word2Vec, offering a rich, nuanced view of word meanings 
Named entity recognition is an NLP task that identifies and classifies key information (entities) in text into predefined categories. Entity Types: Common categories include names of people, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.
A language model predicts the likelihood of a sequence of words. It's a statistical tool that helps computers understand human language 
Types of language models include: Statistical models, Rule based models, Neural Network based models Typical use cases include Language Translation, Text generation, Speech predictions, etc. 
Text generation models are AI-powered tools designed to automatically produce human-like text it is Used to simulate human writing or speech patterns for various applications One popular example of this is ChatGPT   
Large language models are trained on terabytes of text data from which they identify patterns in language to predict and generate subsequent text sequences but remember that their code mechanism is no different than small scale text generation.    
Large Language models try to mimic human conversation by making the model keep context whilst ensuring the generated text remains contextually relevant over longer stretches of content the models are trained to ensure that they do not lose track of the topic or narrative thread 
LLMs have a tendency to hallucinate and generate incorrect or misleading content sometimes the model provide factually inaccurate and unreliable information. Furthermore, the inherent biases in training data can lead to biased outputs. There are also many ethical concerns such as using an LLM model to generate fake news or misleading content online.  
An example of an LLM model is ChatGPT which uses generative pretrained transformed architecture to understand and generate human like text, enabling intuitive conversational experiences. It is used in various domains like customer service education creative industries and programming.   
APIs also known as application programming interface is used with LLM models like OpenAI GPT to call the model on NLP tasks such as question answering, summarizing, text generation and sentiment analysis and more. These APIs can also be used on models like DALL-E which uses a transformer model to encode the text and then decode with a CNN neural network. Other examples of LLM APIs include the use of API to call upon the functionalities of Codex which is designed specifically for programming tasks and generating codes in any language upon natural language prompts. A final example includes the use of Whisper API which has the ability to transcribe spoken language in a highly accurate and context-aware manner.  
A CNN also known as Convolutional Neural Network is a type of deep learning neural network model primarily used in processing data with grid-like topology such as image data. It is used for the purpose of image recognition and classification tasks. The model structure is Composed of layers that automatically and adaptively learn spatial hierarchies of features from input images.   
Applications of CNN model includes Image and Video Recognition, Image Analysis & Classification, Medical Image Analysis, Self-Driving Cars.     
The CNN model architecture has two main components of which the first part is the feature extraction part and then the second part corresponds to the task at hand such as classification or generation or identification. The extraction part of the architecture can include multiple layers depending on the complexity and requirements of the project. But in general the feature extraction part of the model can be broken down into 3 steps, the first is preparing the data for the input. The input layer receives the raw pixel of the image. The second part is the convolutional layer, this is where most of the work happens as the convolutional layers apply a number of filters to the input to create a feature map. The third step involves reducing the spatial size of the input volume for the next convolutional layer and this is done with the help of pooling layers. the vector which is extracted from the input is then passed onto a fully connected layer. And then the results from the fully connected layer is passed onto the output layer where the image is classified with the help of a softmax function.     
The input layer is where the CNN model receives the images as raw pixels in 3D formats. The image has to be processed in a certain way for this to be taken in by the model and often requires padding or other methods of processing. 
The Convolutional layer applies a number of filters to the input to create feature maps. Identifies features like edges, textures, etc.
The Activation layer Introduces non-linearity to the system. Helps the neural network learn complex patterns. 
The pooling layers Reduces the spatial size of the input volume for the next convolutional layer
The fully connected layer is where After several convolutional and pooling layers, the high-level reasoning is done via fully connected layers. The fully connected layers use the extracted features for classification or regression tasks
The key characteristics of a CNN model includes parameter sharing, local connectivity and depth. Parameter sharing reduces the number of parameters enabling the network to be deeper with fewer parameters this is because the weights from all the neurons are shared in one feature map. Local connectivity refers to a concept in CNN where the neurons are connected to only a subset of the input image This helps the neurons to focus on local regions and capture high quality spatial hierarchies. Dept refers to the complexity of the model structure the deeper the layers the more features are collected from the input image. 
The images are preprocessed using a lot of techniques some of them are: resizing, rescaling, cropping, padding, color space conversion. 
We Modify Dataset for Broader Representation such as using data augmentation techniques which includes rotation, translation, zooming, flipping, etc. The reason why this is important is because If a model learns the representation of an object like for instance a pineapple, but only learns the model from its side view, it won’t be able to detect one when its upside down To prevent such scenarios, we modify existing image datasets so our model can detect them no matter which orientation they are in. Creates a more diverse set of training images, helping the model generalize better. Sometimes Synthetic Data Generation is also required for Creating new, artificial images using various algorithms in order to balance the data and augment underrepresented classes. All of this is also part of the image processing.      
In a CNN neural network sliding window refers to the  filter which is moved across the input image (or matrix) step by step to create the feature map. This process is often referred to as the sliding  
When the filter is applied to the input image and the window slide across the image, At each position, the filter performs element-wise multiplication with the part of the image it covers before summing them up into a single output pixel in the feature map (also called the convolved feature). 
The values in the filters are learned during the training of the network. Initially, they are set randomly. Multiple filters are often used in each convolution layer, each detecting different features and then the weights in the filter changes as the model learns from the patterns.   
A feature map represents certain features of the input image, such as edges or textures which is extracted with the help of filters. This process is repeated across the entire image and the neural networks share the weights learned from the image in one feature map.   
When the filter is applied to the input image, the filter can move a certain number of pixels each time (called a stride). A larger stride means the filter jumps over more pixels at each step and produces a smaller feature map.
Sometimes, padding is added to the input image. What this means is that to allow the filter to fit properly at the edges, zeros are added to the sides. This helps in controlling the spatial size of the output feature map.   
Pooling layers reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to decrease computational load and  memory usage Similar to convolutional layers, a pooling layer has a window size that moves across the feature map with a certain stride     
There are many types of pooling layers The most common pooling method is max pooling, which takes the maximum value in each window of the feature map. Whilst Average pooling, taking the average value.  
forward pass is the process through which Input data is passed through the convolutional and pooling layers. Each layer applies its filters and activation functions to process the data  
After each convolutional and pooling layer, an activation function like ReLU is applied to introduce non linearity for the model to be able to map more complex features from the input images. But why is this important, because imagine that the image is being digitized and so essentially the features of the image are being translated to numbers and in order for the model to do this it employs lots of mathematical equations to transform the features from its pixel form to its numerical representation. Therefore the introduction of a non-linear equation allows the model to map the features of the images to more complex numerical relations.   
Once the input image has been processed through the convolutional and the pooling layers, and passed on from the fully connected layer to the output layer. The prediction from the network is then compared to the actual target image. And the differences are compared with the loss function. This process is called loss calculation. 
In an neural network, back propagation is the process through which the model learns from its loss calculation. The model learns by looking back on its error and then adjusting the weights in each layer accordingly to get the best combination of weights which produce the least amount of error. 
Once the back propagation is complete the model will optimize itself with optimization algorithms which updates the weight in the neural network.  
Factors Influencing Computational Cost of a CNN are Size of Input Data The resolution and dimensions of the input data, Number of Layers: More layers in a CNN mean more computations, Number of Filters in Convolutional Layers: The number of filters (and their size) in each convolutional layer determines the number of operations required to compute and can increase the overall computation cost. A larger stride reduces the spatial dimensions of the output feature maps, thus reducing the number of computations. Pooling layers reduce the spatial dimensions of the feature maps, which can decrease computational cost in subsequent layers. Fully Connected Layers typically found near the end of CNNs, can have a high computational cost, especially if they have a large number of neurons.    
The computational challenges of a CNN model includes: memory usage for storing the model weights, training time can be much longer with the increase in model complexity along with the size of the dataset. Inference time is also much longer for the model to predict on new data at real-time.
Some strategies to reduce the computational cost of training a CNN model is Network Pruning which Removes redundant or non-contributing neurons and connections to reduce network size without significantly affecting performance. Transfer Learning which uses a pre-trained network and fine-tunes it for a specific task can save computational resources, as the network has already learned general features. Some architectures, like MobileNets or EfficientNets, are designed specifically to reduce computational costs while maintaining high performance. Distributing the training process across multiple machines can also help manage the computational load.    
"In the context of Convolutional Neural Networks a ""backbone"" refers to the base part of the network that is responsible for extracting features from the input data. The primary role of the backbone in a CNN is to extract meaningful features from the input data This involves capturing various aspects such as edges, textures, and patterns. CNN backbones typically consist of multiple layers, where each layer builds upon the features extracted by the previous layers in a Hierarchical Process. This hierarchical structure allows the network to learn complex and abstract representations of the data     "
The reason why Backbones are important is because they help facilitate transfer learning, increase the efficiency and performance, increase adaptability in terms of using backbones that can be adapted to different scales and complexities of tasks. For instance, lighter backbones like MobileNet are used for applications where speed and low memory footprint are crucial while more complex backbones like ResNet are used for tasks where accuracy is more important    
The names of some CNN architectures are: VGG (Visual Geometry Group) VGG models, particularly VGG16 and VGG19, are known for their deep architectures with consecutive convolution layers. They are straightforward but computationally intensive. ResNet (Residual Networks) uses residual learning to alleviate the vanishing gradient problem in very deep networks. MobileNet architectures use depthwise separable convolutions to reduce the model size and computational complexity while maintaining high performance        
Sampling rate refers to the number of samples of audio carried per second For instance, a common sampling rate for music is 44.1 kHz, which means 44,100 samples per second Higher sampling rates can capture more detail but require more data 
Digital Signal Processing is the process through which various techniques like filtering, modulation, sampling, and quantization is used to manipulate signals form audio data to improve their quality or extract information Since audio files can be large, they are often compressed 
Fourier Transform and Spectral Analysis are mathematical tools that allow the decomposition of a sound wave into its constituent frequencies It's fundamental in understanding the spectral content of audio data.   
The Challenges in Speech Recognition include understanding Accents and Dialects extracting Background Noise, Speaker Variability which refers to Individual differences in pitch, tone, and speaking style which can affect recognition accuracy, Homophones and Contextual Understanding and Language and Linguistic Diversity     
"Traditional methods of Automatic Speech Recognition includes Feature Extraction where The first step involves processing the raw audio to extract meaningful features, like Mel-frequency cepstral coefficients (MFCCs). These features are designed to represent the phonetic content of speech and then Acoustic Modeling which involves modeling the relationship between the audio features and the phonetic units (like phonemes) in speech and finally Language Modeling where the system uses a language model, usually based on probabilities, to predict the likelihood of certain word sequences. This helps in determining the most probable words from the phonetic sequences. Once these steps are complete the decoder combines the outputs from the acoustic and language models to determine the most likely word sequence Finally, the system might include some post-processing to handle things like punctuation insertion or
Capitalization.      "
Modern ASR Approaches include using End-to-End Deep Learning models that can learn directly from audio to text, without the need for separate acoustic and language models. Techniques like Convolutional Neural Networks (CNNs) for feature extraction and Recurrent Neural Networks (RNNs) or Transformers for capturing sequential dependencies in speech are common   
Wav2Vec is a deep learning model which represents a paradigm shift towards end-to-end learning directly from raw audio data Wav2Vec is designed to learn speech representations directly from raw waveform, simplifying the speech recognition pipeline. It learns powerful representations of speech by predicting parts of the audio waveform, not seen during training, based on the context provided by other parts of the waveform. The Wav2Vec model architecture comprises two main components a convolutional feature encoder that processes raw audio a context network that aggregates information over time.     
The model is first pre-trained on a large unlabelled dataset. This pre-training allows it to learn general features of speech. It is then fine-tuned on a smaller labelled dataset for specific speech recognition tasks 
Sequence data refers to a collection of elements arranged in a specific order where the arrangement is significant This data type is common in various domains like natural language (words in a sentence) time series (stock market prices) biological data (DNA sequence). Characteristics of these data include Ordered: Each element in the sequence is positioned in a specific order, Temporal or Spatial Dependency: Elements may have dependencies or relationships with preceding or succeeding elements.  
Sequence learning is a type of machine learning where the algorithm learns from sequence data The goal is to understand the structure or features of the sequence to make predictions about future elements or classify the sequence into different categories.
Some examples of Sequence Learning Approaches are Recurrent Neural Networks (RNNs): Designed to handle sequential data by having loops in them, allowing information to persist. Long Short-Term Memory (LSTM): An advanced form of RNNs that can learn long-term dependencies in sequence data. Transformers which uses self-attention mechanisms to handle sequences.    
Challenges in Sequence Learning include Long-Range Dependencies which refers to the situation where current elements in a sequence are dependent on elements that appeared much earlier in the sequence. Traditional sequence models like basic Recurrent Neural Networks (RNNs) struggle to learn these dependencies due to the problem of vanishing gradients but Long Short-Term Memory (LSTM) Architectures are specifically designed to mitigate the vanishing gradient problem and better capture long-term dependencies. In transformers the attention mechanism allows the model to focus on relevant parts of the input sequence, regardless of their position, making them highly effective for long-range dependencies.  
"The vanishing gradient problem refers to when the influence of a given input decreases exponentially over time Storing information over long sequences requires significant memory, which can be a limitation for many models. These issues arise during the backpropagation process, which is used to update the network's weights based on the gradient of the loss function. The gradients of the loss function become increasingly smaller as they are propagated back through each layer during training. The primary cause is the multiplication of gradients through many layers or time steps. If these gradients are small (less
than 1), their repeated multiplication makes them exponentially smaller This is often exacerbated by certain activation functions like the sigmoid or tanh, which have derivatives that can be very small. As a result When gradients vanish, the weights in the early layers of the Network receive very tiny updates or none at all This makes the training process extremely slow and can result in the network not learning the long-range dependencies in the data.   "
the gradients can grow exponentially large as they are propagated back through the layers, causing very large updates to the network weights This typically occurs when the gradients are greater than 1, and their repeated multiplication leads to exponentially larger values It can be exacerbated by the network architecture, choice of activation function, and data characteristics Exploding gradients can lead to numerical instability and wildly oscillating network behavior The model weights can become so large that the model fails to converge. A machine learning model reaches convergence when it achieves a state during training in which loss settles to within an error range around the final value. 
The Use of Gated Architectures: LSTMs and GRUs are designed to mitigate this problem through their gating mechanisms Alternative Activation Functions: ReLU (Rectified Linear Unit) and its variants help Network Initialization and Batch Normalization 
The Use of Gradient Clipping which involves scaling down gradients when they exceed a certain threshold Weight Regularization: Techniques like L1(Lasso) or L2(Ridge) regularization can help in keeping the weights small. Note: L1 regularization tends to produce sparse solutions by driving some coefficients to zero, while L2 regularization encourages smaller coefficients overall without forcing them to be exactly zero.
Long Short-Term Memory (LSTM) networks are a special kind of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequence data LSTMs are particularly well-suited for classifying, processing, and making predictions based on time-series data.
"LSTM networks are composed of a series of modules, often referred to as ""cells."" Each cell in an LSTM network is designed to remember values over arbitrary time intervals and to regulate the flow of information This is the ""memory"" part of the LSTM. It also has a gate component Gates are a way to optionally let information through  They are composed of a sigmoid neural net layer. The sigmoid layer outputs numbers between 0 and 1, describing how much of each component should be let through. An output of 0 means ""let nothing through,"" while an output of 1 means ""let everything through."" There are 3 types of these gates in the architecture. Forget Gate: This gate decides what information should be thrown away or kept Input Gate: The input gate decides what new information to store in the cell state Output Gate: The output gate decides what the next hidden state should be. Sequence data enters the LSTM cell from the input layer Inside the LSTM cell, the input data interacts with the cell state and the gates, resulting in an updated cell state and a hidden state The output of each LSTM cell is based on the cell's current state and the input it has just processed. The output can be used directly for predictions or fed into the next LSTM cell in the sequence.  "
Typically, input to an LSTM is a tensor of shape: [batch_size, time_steps, features] Backpropagation Through Time (BPTT): LSTMs are trained using a variant of backpropagation called BPTT, where gradients are calculated and propagated back through time to update the weights Gradient Clipping: Often used to prevent exploding gradients in LSTMs by setting a threshold value  Often Gradient Clipping is used to prevent exploding gradients in LSTMs by setting a threshold value. 
Algorithms like Adam, RMSprop are typically used for better convergence in training LSTM networks. Dropouts are applied to inputs and recurrent connections to prevent overfitting. Choosing the right number of layers, units, and learning rate is crucial and often requires extensive experimentation. 
A Bidirectional Long Short-Term Memory (Bi-LSTM) is an extension of the traditional Long Short-Term Memory (LSTM) network. It improves an LSTM’s understanding of context in sequence learning applications. A Bi-LSTM consists of two LSTM layers that process the data in opposite directions: one forward and one backward. The forward LSTM layer processes the sequence from start to end, while the backward LSTM layer processes it from end to start. This dual structure allows the network to capture information from both past (backward) and future (forward) states of the sequence At any given point in the sequence, the Bi-LSTM has complete, contextual information about all points before and after it. The outputs of the forward and backward LSTMs are combined at each time step. Like standard LSTMs, Bi-LSTMs are trained using Backpropagation Through Time The gradients from both directions are calculated separately and then combined to update the model parameters The training is computationally more intensive than standard LSTMs due to the doubled number of LSTM layers. 
Sequence-to-Sequence (seq2seq) models are a category of neural network architectures designed to transform a given sequence of elements, such as words in a sentence, into another sequence. These models are particularly effective in tasks where the input and output sequences can be of different lengths
The Seq2seq model uses encode and decoder architecture. Where the encoder Processes the input sequence and compresses the information into a context vector. The decoder then takes the context vector and generates the output sequence.  
This is a fixed-length representation of the entire input sequence and acts as the bridge between the encoder and decoder. It's supposed to capture the essence of the input sequence. 
