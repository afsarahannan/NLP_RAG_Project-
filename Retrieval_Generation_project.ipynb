{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afsarahannan/NLP_RAG_Project-/blob/main/Retrieval_Generation_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTwpDNBnorUT"
      },
      "source": [
        "# RAG Project: ColBert and GPT3.5\n",
        "\n",
        "If you're working in Google Colab, we recommend selecting \"T4 GPU\" as your hardware accelerator in the runtime settings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the environment, loading necessary libraries, load the datset"
      ],
      "metadata": {
        "id": "Mga1NykdObvK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl_YBBPTo5AZ",
        "outputId": "f1e2474a-8838-461c-d52a-d083c9ef7920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: cannot change to 'ColBERT/': No such file or directory\n",
            "Cloning into 'ColBERT'...\n",
            "remote: Enumerating objects: 2662, done.\u001b[K\n",
            "remote: Counting objects: 100% (1165/1165), done.\u001b[K\n",
            "remote: Compressing objects: 100% (364/364), done.\u001b[K\n",
            "remote: Total 2662 (delta 908), reused 848 (delta 801), pack-reused 1497\u001b[K\n",
            "Receiving objects: 100% (2662/2662), 2.03 MiB | 12.47 MiB/s, done.\n",
            "Resolving deltas: 100% (1667/1667), done.\n"
          ]
        }
      ],
      "source": [
        "!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git\n",
        "import sys; sys.path.insert(0, 'ColBERT/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FmBi2UT5pxb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd1aa083-fb23-49d5-b1a8-ae0e7aecfc80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n",
            "Obtaining file:///content/ColBERT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitarray (from colbert-ai==0.2.19)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Collecting datasets (from colbert-ai==0.2.19)\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19) (2.2.5)\n",
            "Collecting git-python (from colbert-ai==0.2.19)\n",
            "  Downloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
            "Collecting python-dotenv (from colbert-ai==0.2.19)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ninja (from colbert-ai==0.2.19)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19) (4.66.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19) (4.35.2)\n",
            "Collecting ujson (from colbert-ai==0.2.19)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting torch==1.13.1 (from colbert-ai==0.2.19)\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-gpu>=1.7.0 (from colbert-ai==0.2.19)\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->colbert-ai==0.2.19) (4.9.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->colbert-ai==0.2.19)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->colbert-ai==0.2.19)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->colbert-ai==0.2.19)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->colbert-ai==0.2.19)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->colbert-ai==0.2.19) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->colbert-ai==0.2.19) (0.42.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (1.25.2)\n",
            "Collecting pyarrow>=12.0.0 (from datasets->colbert-ai==0.2.19)\n",
            "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->colbert-ai==0.2.19)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (3.4.1)\n",
            "Collecting multiprocess (from datasets->colbert-ai==0.2.19)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->colbert-ai==0.2.19) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19) (6.0.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19) (8.1.7)\n",
            "Collecting gitpython (from git-python->colbert-ai==0.2.19)\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai==0.2.19) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai==0.2.19) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai==0.2.19) (0.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.19) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.19) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.19) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.19) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.19) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.19) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->colbert-ai==0.2.19) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->colbert-ai==0.2.19) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->colbert-ai==0.2.19) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->colbert-ai==0.2.19) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->colbert-ai==0.2.19) (2024.2.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython->git-python->colbert-ai==0.2.19)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->colbert-ai==0.2.19) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->colbert-ai==0.2.19) (2023.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->colbert-ai==0.2.19) (1.16.0)\n",
            "Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: ninja, faiss-gpu, bitarray, ujson, smmap, python-dotenv, pyarrow, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, dill, nvidia-cudnn-cu11, multiprocess, gitdb, torch, gitpython, git-python, datasets, colbert-ai\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 10.0.1\n",
            "    Uninstalling pyarrow-10.0.1:\n",
            "      Successfully uninstalled pyarrow-10.0.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "  Running setup.py develop for colbert-ai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitarray-2.9.2 colbert-ai-0.2.19 datasets-2.17.0 dill-0.3.8 faiss-gpu-1.7.2 git-python-1.0.3 gitdb-4.0.11 gitpython-3.1.41 multiprocess-0.70.16 ninja-1.11.1.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pyarrow-15.0.0 python-dotenv-1.0.1 smmap-5.0.1 torch-1.13.1 ujson-5.9.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "try: # When on google Colab, let's install all dependencies with pip.\n",
        "    import google.colab\n",
        "    !pip install -U pip\n",
        "    !pip install -e ColBERT/['faiss-gpu','torch']\n",
        "except Exception:\n",
        "  import sys; sys.path.insert(0, 'ColBERT/')\n",
        "  try:\n",
        "    from colbert import Indexer, Searcher\n",
        "  except Exception:\n",
        "    print(\"You're running outside Colab, please make sure you install ColBERT in conda. Conda is recommended.\")\n",
        "    assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N0jxbVar4kln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5acee12-8a8e-4455-b711-dd7b4fd35daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ],
      "source": [
        "import colbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xQg9A-dtp1nB"
      },
      "outputs": [],
      "source": [
        "from colbert import Indexer, Searcher\n",
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert.data import Queries, Collection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the csv dataset here\n",
        "import pandas as pd\n",
        "queries = pd.read_csv(\"/content/queries.csv\", header = None ,sep='\\t')\n",
        "answers = pd.read_csv(\"/content/answers.csv\", header = None ,sep='\\t')"
      ],
      "metadata": {
        "id": "grY5rJ27wVs9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions =[x for x in queries[0]]\n",
        "answer_index = [x for x in queries[1]]\n",
        "answer_index_integers = [[int(num) for num in sub_string.split(',')] for sub_string in answer_index]\n",
        "answer_text = [x for x in answers[0]]\n",
        "print(f\"There are {len(questions)} questions and {len(answer_text)} answers in this notebook.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpkG4eRoyYmm",
        "outputId": "a954c082-2af4-4e6e-ae0f-06620eb88335"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 72 questions and 83 answers in this notebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJdAAbDu7PZ"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "\n",
        "Below, the `Indexer` take a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from this index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vKAdVN5MvDKD"
      },
      "outputs": [],
      "source": [
        "nbits = 2   # encode each dimension with 2 bits\n",
        "doc_maxlen = 300 # truncate passages at 300 tokens\n",
        "# max_id = 10000\n",
        "\n",
        "index_name = f'ML_Edge.{nbits}bits'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_name"
      ],
      "metadata": {
        "id": "OOvGGR4U6Mld",
        "outputId": "bcb758da-703a-428a-ae08-c21e421b10f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ML_Edge.2bits'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orKfQRmQv46u"
      },
      "source": [
        "Now run the `Indexer` on the collection subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JRiOnzxtwI0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "b6260d48908f49d18ae04091e2f100ea",
            "7100d4f588c142f2ad2962990f014d37",
            "3f6490167e7f4417be46d580b8dbe544",
            "d60a4b8aa7b44b4a81e52e303b449f89",
            "9ea11a7c5b3a4b0b9a7274596e5443be",
            "415983219610460fae40733a4ed333b4",
            "dd8f8ebd73904b03bd4daef9cc1cc540",
            "075705dddcc0494db559efedbc037761",
            "d26988f2e49b4045aca8bf981f9b5fe6",
            "704b56a89e524829b649d72688d6e4e5",
            "93bc3fbb8dde4a94bda3efc9ac0b86da"
          ]
        },
        "outputId": "ec62b991-990a-4512-8c33-11b468c5afcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6260d48908f49d18ae04091e2f100ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[Feb 15, 07:14:32] #> Creating directory /content/experiments/notebook/indexes/ML_Edge.2bits \n",
            "\n",
            "\n",
            "#> Starting...\n",
            "#> Joined...\n"
          ]
        }
      ],
      "source": [
        "checkpoint = 'colbert-ir/colbertv2.0'\n",
        "\n",
        "with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use\n",
        "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n",
        "                                                                                # Consider larger numbers for small datasets.\n",
        "\n",
        "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
        "    indexer.index(name=index_name, collection=answer_text, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY6_D523yBFB"
      },
      "source": [
        "## Search\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "j3x_FnVnyB0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961ea3ee-51e0-41cd-8f91-67444cedf9d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 15, 07:18:51] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Feb 15, 07:18:52] #> Loading codec...\n",
            "[Feb 15, 07:18:52] #> Loading IVF...\n",
            "[Feb 15, 07:18:52] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 15, 07:19:31] #> Loading doclens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2641.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 15, 07:19:31] #> Loading codes and residuals...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 188.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 15, 07:19:31] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Feb 15, 07:20:08] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        }
      ],
      "source": [
        "# Create the searcher\n",
        "with Run().context(RunConfig(experiment='notebook')):\n",
        "  searcher = Searcher(index=index_name, collection=answer_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#These are the type of sample questions we can ask the retriever\n",
        "questions"
      ],
      "metadata": {
        "id": "tTmJxKWthVoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "3JYA0N22yIeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff9ab92-dac6-4e72-f950-cef838187135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: what is machine learning ?\n",
            "\t [1] \t\t 30.1 \t\t Machine learning is a branch of Computer Science which focuses on the use of data and algorithms to imitate the way humans learn \n",
            "\t [2] \t\t 28.3 \t\t Machine Learning is an important field of data science because there is too much data in the world for humans to process and Classical Machine Learning is dependent on human Intervention which is a sub-field of AI that uses algorithms trained on data to produce adaptable models to perform tasks  \n",
            "\t [3] \t\t 27.9 \t\t Machine learning is used when the task is simple and structured enough for Machine Learning models, When computational resources are minimal and When model interpretation is required\n"
          ]
        }
      ],
      "source": [
        "query = questions[0] # try with an in-range query or supply your own\n",
        "print(f\"Question: {query}\")\n",
        "\n",
        "# Find the top-3 passages for this query\n",
        "results = searcher.search(query, k=3)\n",
        "\n",
        "# Print out the top-k retrieved passages\n",
        "for passage_id, passage_rank, passage_score in zip(*results):\n",
        "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this code chunk is used to retrieve all the top answers for one query, will be used again during the time of inference\n",
        "\n",
        "all_data = []\n",
        "for passage_id, passage_rank, passage_score in zip(*results):\n",
        "  data = searcher.collection[passage_id]\n",
        "  all_data.append(data)\n",
        "\n",
        "retrieved_response = ''.join(all_data)"
      ],
      "metadata": {
        "id": "3mpZBLvyTOSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the project is making use of the ColBert model to extract and store information as a txt file  "
      ],
      "metadata": {
        "id": "wdeQQHZGUv0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I need the context list from the ColBert Model\n",
        "context = []\n",
        "\n",
        "for query in questions:\n",
        "  results = searcher.search(query, k=3)\n",
        "  intermediate_data = []\n",
        "  for passage_id, passage_rank, passage_score in zip(*results):\n",
        "    data = searcher.collection[passage_id]\n",
        "    intermediate_data.append(data)\n",
        "  context.append(intermediate_data)\n",
        "  intermediate_data=[]\n",
        "\n",
        "context_compiled = [\"\".join(text) for text in context]"
      ],
      "metadata": {
        "id": "NRwrCNWriCl-"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the context from ColBert model as an external file\n",
        "# File path to save the CSV file\n",
        "import csv\n",
        "file_path = '/content/context.txt'\n",
        "\n",
        "# Writing the list to a text file\n",
        "with open(file_path, 'w') as file:\n",
        "    for item in context:\n",
        "        file.write(\"%s\\n\" % item)"
      ],
      "metadata": {
        "id": "VEllS9QbBNaR"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "mK6ouJpM7X2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation with pretrained t5-base model"
      ],
      "metadata": {
        "id": "puoVf0KJaeh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the transformers library\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install rouge\n"
      ],
      "metadata": {
        "id": "7saVRpLVaDO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import evaluate\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "import transformers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "SRWa7CqBVmGt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload the dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "queries = pd.read_csv(\"/content/queries.csv\", header = None ,sep='\\t')\n",
        "answers = pd.read_csv(\"/content/answers.csv\", header = None ,sep='\\t')\n",
        "\n",
        "# File path of the text file\n",
        "file_path = '/content/context.txt'\n",
        "\n",
        "# Reading the contents of the text file\n",
        "with open(file_path, 'r') as file:\n",
        "    context = file.read()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EVuhq0pZZ-Ra"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list of all the questions\n",
        "questions =[x for x in queries[0]]\n",
        "\n",
        "\n",
        "#index of all the answer corresponding to the question\n",
        "answer_index = [x for x in queries[1]]\n",
        "answer_index_integers = [[int(num)-1 for num in sub_string.split(',')] for sub_string in answer_index]\n",
        "\n",
        "#passage of the answers\n",
        "answer_text = [x for x in answers[0]]\n"
      ],
      "metadata": {
        "id": "dK2uN1FEawBD"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine all the answers corresponding to the questions in one list\n",
        "answer = [[answer_text[x] for x in sublist]for sublist in answer_index_integers]\n",
        "answer_compiled = [\"\".join(text) for text in answer]"
      ],
      "metadata": {
        "id": "6NgIvi9kb2PC"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME =\"t5-base\"\n",
        "TOKENIZER = T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
        "MODEL = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
        "OPTIMIZER = Adam(MODEL.parameters(), lr=0.0001)\n",
        "Q_LEN = 150   # Question Length\n",
        "T_LEN = 200    # Target Length\n",
        "BATCH_SIZE = 3\n",
        "DEVICE = \"cpu\""
      ],
      "metadata": {
        "id": "pm6jjoMuVxld"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QA_Dataset(Dataset):\n",
        "    def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.q_len = q_len\n",
        "        self.t_len = t_len\n",
        "        self.data = dataframe\n",
        "        self.questions = self.data[\"questions\"]\n",
        "        self.context = self.data[\"context\"]\n",
        "        self.answer = self.data[\"answer\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        context = self.context[idx]\n",
        "        answer = self.answer[idx]\n",
        "\n",
        "        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n",
        "                                                    truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
        "        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\",\n",
        "                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
        "\n",
        "        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n",
        "        labels[labels == 0] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
        "            \"labels\": labels,\n",
        "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "A3eNZsBAX0an"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions_df = pd.DataFrame(questions, columns=['questions'])\n",
        "answer_df = pd.DataFrame(answer_compiled, columns=['answer'])\n",
        "context_df = pd.DataFrame(context_compiled, columns=['context'])\n",
        "\n",
        "data = pd.concat([questions_df, context_df,answer_df], axis=1)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "QpLAmZ0MYJRB",
        "outputId": "aea5408a-19a9-4384-d5a3-aba478f0fd81"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            questions  \\\n",
              "0                          what is machine learning ?   \n",
              "1    Why do we need to know about machine learning ?    \n",
              "2                              What is deep learning?   \n",
              "3   How is deep learning different from machine le...   \n",
              "4   When do we use deep learning or machine learni...   \n",
              "..                                                ...   \n",
              "67                              What is SentenceBert    \n",
              "68                                      What is GloVe   \n",
              "69           What is a named entity recognition (NER)   \n",
              "70                          What is a language model    \n",
              "71   What are the different types of language models    \n",
              "\n",
              "                                              context  \\\n",
              "0   Machine learning is a branch of Computer Scien...   \n",
              "1   Machine Learning is an important field of data...   \n",
              "2   Deep learning is a subset of Machine learning ...   \n",
              "3   Deep learning is a subset of Machine learning ...   \n",
              "4   Machine learning is used when the task is simp...   \n",
              "..                                                ...   \n",
              "67   SentenceBert adapts the powerful BERT model t...   \n",
              "68  GloVe builds a co-occurrence matrix that recor...   \n",
              "69  Named entity recognition is an NLP task that i...   \n",
              "70  A language model predicts the likelihood of a ...   \n",
              "71  Types of language models include: Statistical ...   \n",
              "\n",
              "                                               answer  \n",
              "0   Machine learning is a branch of Computer Scien...  \n",
              "1   Machine Learning is an important field of data...  \n",
              "2   Deep learning is a subset of Machine learning ...  \n",
              "3   Unlike traditional ML, it does not require man...  \n",
              "4   Machine learning is used when the task is simp...  \n",
              "..                                                ...  \n",
              "67   SentenceBert adapts the powerful BERT model t...  \n",
              "68  GloVe builds a co-occurrence matrix that recor...  \n",
              "69  Named entity recognition is an NLP task that i...  \n",
              "70  A language model predicts the likelihood of a ...  \n",
              "71  Types of language models include: Statistical ...  \n",
              "\n",
              "[72 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47ce8ad0-c1a4-440a-a83f-c18555793c9b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questions</th>\n",
              "      <th>context</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what is machine learning ?</td>\n",
              "      <td>Machine learning is a branch of Computer Scien...</td>\n",
              "      <td>Machine learning is a branch of Computer Scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why do we need to know about machine learning ?</td>\n",
              "      <td>Machine Learning is an important field of data...</td>\n",
              "      <td>Machine Learning is an important field of data...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is deep learning?</td>\n",
              "      <td>Deep learning is a subset of Machine learning ...</td>\n",
              "      <td>Deep learning is a subset of Machine learning ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How is deep learning different from machine le...</td>\n",
              "      <td>Deep learning is a subset of Machine learning ...</td>\n",
              "      <td>Unlike traditional ML, it does not require man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>When do we use deep learning or machine learni...</td>\n",
              "      <td>Machine learning is used when the task is simp...</td>\n",
              "      <td>Machine learning is used when the task is simp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>What is SentenceBert</td>\n",
              "      <td>SentenceBert adapts the powerful BERT model t...</td>\n",
              "      <td>SentenceBert adapts the powerful BERT model t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>What is GloVe</td>\n",
              "      <td>GloVe builds a co-occurrence matrix that recor...</td>\n",
              "      <td>GloVe builds a co-occurrence matrix that recor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>What is a named entity recognition (NER)</td>\n",
              "      <td>Named entity recognition is an NLP task that i...</td>\n",
              "      <td>Named entity recognition is an NLP task that i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>What is a language model</td>\n",
              "      <td>A language model predicts the likelihood of a ...</td>\n",
              "      <td>A language model predicts the likelihood of a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>What are the different types of language models</td>\n",
              "      <td>Types of language models include: Statistical ...</td>\n",
              "      <td>Types of language models include: Statistical ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>72 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47ce8ad0-c1a4-440a-a83f-c18555793c9b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-47ce8ad0-c1a4-440a-a83f-c18555793c9b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-47ce8ad0-c1a4-440a-a83f-c18555793c9b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-44a489fc-e8fc-47f7-83e2-15f084515a5b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-44a489fc-e8fc-47f7-83e2-15f084515a5b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-44a489fc-e8fc-47f7-83e2-15f084515a5b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6c664421-d54b-4364-b642-efef6bfd28a9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6c664421-d54b-4364-b642-efef6bfd28a9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 72,\n  \"fields\": [\n    {\n      \"column\": \"questions\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"When do we use deep learning or machine learning for a task?\",\n          \"What is tokenization in NLP ?\",\n          \"What is online learning ? \"\n        ],\n        \"num_unique_values\": 72,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"Any evaluation we make has to be an objective one There are several metrics to evaluate a model, for example accuracy precision recall specificity and f1-score these are used in the case of a classification problem.  A perfect model fit can be achieved By first constructing good evaluation metrics to give us feedback on model performance Then tuning hyper-parameters till the performance improves across the board To avoid overfitting/underfitting To ensure a model\\u2019s prediction are reliable To evaluate how a model may work in real-world scenarios Aid engineers to make decisions about model deployment \",\n          \"Machine learning is a branch of Computer Science which focuses on the use of data and algorithms to imitate the way humans learn Machine Learning is an important field of data science because there is too much data in the world for humans to process and Classical Machine Learning is dependent on human Intervention which is a sub-field of AI that uses algorithms trained on data to produce adaptable models to perform tasks  Machine learning is used when the task is simple and structured enough for Machine Learning models, When computational resources are minimal and When model interpretation is required\",\n          \"There are many instances where NLP is utilized to provide value to both companies and individuals. Some of these examples are Classification: Sentiment Analysis, Token classification, gender text alignment Sequence Generation: ASR, QA, Fill-Mask, NSP, Translation and Multiple Choice: Choosing between several candidates. Feature Engineering is the process of converting raw data into a numerical format that algorithms can utilize for prediction or classification Feature engineering in NLP is crucial for transforming text into  a structured, machine-readable form there are many ways in which this can be achieved some of the methods include Bag of words \\u2013 countvectorizer, term frequency inverse document frequency TF-IDF, GloVe(Global vectors for word representation), these are the statistical ways in which texts are converted into numerical version of itself. This process can also be achieved with the help of neural networks like Word2Vec embedding, SentenceBERT.   Another stage of preprocessing in NLP is stopword and punctuation removal. Stopwords are commonly used words (such as 'the', 'is', 'at') that are filtered out before processing since they add noise without informative content this helps with focused analysis and faster processing Caution is advised as some stopwords can change the meaning of a sentence (e.g., 'not'). Punctuation marks are often removed during text preprocessing because: They can be irrelevant for understanding the meaning of texts, especially in models focusing on individual words. However, in certain contexts like sentiment analysis, exclamation points or question marks can carry sentiment and should be preserved.\"\n        ],\n        \"num_unique_values\": 70,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"Once the stopwords and punctuations are removed from the sentences the next stage in NLP preprocessing is normalization and lemmatization of the text Normalization standardizes text, such as converting to lowercase, while lemmatization reduces words to their base or dictionary form this Helps in reducing the number of unique tokens in the text  Lemmatization takes into account the morphological analysis of the words, aiming to remove inflectional endings only and to return the base or dictionary form of a word \",\n          \" SentenceBert adapts the powerful BERT model to generate embeddings that represent the meaning of entire sentences, not just words By using siamese and triplet network structures, SentenceBert is trained to understand the nuanced differences and similarities between sentences these embeddings excel in tasks requiring deep semantic understanding, such as semantic text similarity, clustering, and information retrieval The trade-off for this depth of understanding is the need for greater computational resources and more complex model fine-tuning     \",\n          \"Machine learning is a branch of Computer Science which focuses on the use of data and algorithms to imitate the way humans learn A computer program is said to learn from experience with respect to some class of tasks and measure  the performance with experience Machine Learning is the field of study that gives the computer the ability to learn without being explicitly programmed. \"\n        ],\n        \"num_unique_values\": 65,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader\n",
        "\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "train_sampler = RandomSampler(train_data.index)\n",
        "val_sampler = RandomSampler(val_data.index)\n",
        "\n",
        "qa_dataset = QA_Dataset(TOKENIZER, data, Q_LEN, T_LEN)\n",
        "\n",
        "train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"
      ],
      "metadata": {
        "id": "DYs6yOx0X0WW"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = 0\n",
        "val_loss = 0\n",
        "train_batch_count = 0\n",
        "val_batch_count = 0\n",
        "\n",
        "for epoch in range(2):\n",
        "    MODEL.train()\n",
        "    for batch in tqdm(train_loader, desc=\"Training batches\"):\n",
        "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "        labels = batch[\"labels\"].to(DEVICE)\n",
        "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
        "\n",
        "        outputs = MODEL(\n",
        "                          input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          labels=labels,\n",
        "                          decoder_attention_mask=decoder_attention_mask\n",
        "                        )\n",
        "\n",
        "        OPTIMIZER.zero_grad()\n",
        "        outputs.loss.backward()\n",
        "        OPTIMIZER.step()\n",
        "        train_loss += outputs.loss.item()\n",
        "        train_batch_count += 1\n",
        "\n",
        "    #Evaluation\n",
        "    MODEL.eval()\n",
        "    for batch in tqdm(val_loader, desc=\"Validation batches\"):\n",
        "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "        labels = batch[\"labels\"].to(DEVICE)\n",
        "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
        "\n",
        "        outputs = MODEL(\n",
        "                          input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          labels=labels,\n",
        "                          decoder_attention_mask=decoder_attention_mask\n",
        "                        )\n",
        "\n",
        "        OPTIMIZER.zero_grad()\n",
        "        outputs.loss.backward()\n",
        "        OPTIMIZER.step()\n",
        "        val_loss += outputs.loss.item()\n",
        "        val_batch_count += 1\n",
        "\n",
        "    print(f\"{epoch+1}/{2} -> Train loss: {train_loss / train_batch_count}\\tValidation loss: {val_loss/val_batch_count}\")"
      ],
      "metadata": {
        "id": "2AynecfWX0CC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8707c17-eb47-483a-9a77-a0dded88c71f"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training batches: 100%|██████████| 19/19 [07:15<00:00, 22.90s/it]\n",
            "Validation batches: 100%|██████████| 5/5 [01:46<00:00, 21.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/2 -> Train loss: 2.906788879319241\tValidation loss: 1.665006732940674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training batches: 100%|██████████| 19/19 [07:12<00:00, 22.75s/it]\n",
            "Validation batches: 100%|██████████| 5/5 [01:28<00:00, 17.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 -> Train loss: 1.975010090360516\tValidation loss: 1.5156640768051148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL.save_pretrained(\"QA_model\")\n",
        "TOKENIZER.save_pretrained(\"QA_tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tXTrT-TfFVi",
        "outputId": "517a5f2c-bbd0-41e6-e44c-1a64d38e3602"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('QA_tokenizer/tokenizer_config.json',\n",
              " 'QA_tokenizer/special_tokens_map.json',\n",
              " 'QA_tokenizer/spiece.model',\n",
              " 'QA_tokenizer/added_tokens.json',\n",
              " 'QA_tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_answer(context, question, ref_answer=None):\n",
        "    inputs = TOKENIZER(question, context, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n",
        "\n",
        "    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
        "    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
        "\n",
        "    outputs = MODEL.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    predicted_answer = TOKENIZER.decode(outputs.flatten(), skip_special_tokens=True)\n",
        "\n",
        "    if ref_answer:\n",
        "        # Load the Bleu metric\n",
        "        bleu = evaluate.load(\"google_bleu\")\n",
        "        score = bleu.compute(predictions=[predicted_answer],\n",
        "                            references=[ref_answer])\n",
        "\n",
        "        print(\"Context: \\n\", context)\n",
        "        print(\"\\n\")\n",
        "        print(\"Question: \\n\", question)\n",
        "        return {\n",
        "            \"Reference Answer: \": ref_answer,\n",
        "            \"Predicted Answer: \": predicted_answer,\n",
        "            \"BLEU Score: \": score\n",
        "        }\n",
        "    else:\n",
        "        return predicted_answer"
      ],
      "metadata": {
        "id": "wG1wcNdGgQKT"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the inference part of the model\n",
        "import random\n",
        "n = random.randint(0,71)\n",
        "\n",
        "context = data['context'][n]\n",
        "question = data['questions'][n]\n",
        "answer = data['answer'][n]\n",
        "\n",
        "predict_answer(context, question, answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "844ae9ea9da343df9386921d66d2a10e",
            "7a692247727b468d917b0f2d4a05741f",
            "24970e00b0ec418a9c0cff1e123842b6",
            "612537e0d11e452f970a6aabd29731d8",
            "6ee740c4417e4230bdb9c19c2e39a2fd",
            "ec38746bdeb84f53a974c28af2025513",
            "cd88c93e099c43afa121842dc6ae0e80",
            "0a7984d5b8944d4f9e0568a5dc622ced",
            "9c698c10e5d242ed8d3c7216c2141d47",
            "752f8b7c5ae3418da491e24ad820766e",
            "bff6b706fb7a47d7b86b243e8b59a85a",
            "e896db0f7f4a43ddbb4a6da4463c8999",
            "12a071c3837a4ce7a97acf61e4bc723e",
            "6acd8862ccea44af9ad4ee48e1cf1a03",
            "a170441616734c10b941cf4338275c4e",
            "88fbb3635e1040ce88ea5da9aaa83ce1",
            "479123f775cf4917b88aa0c87550ee79",
            "f6e05dfb4bb24f459709636f45a3bd7b",
            "42d78426a40d4568b6960badd245d85b",
            "562f7fd0d7fc40fba175b797447142fc",
            "1d7f07882a6547d390ba1787b7a85647",
            "51acb390a0ff45ee982d1b331fb9b918"
          ]
        },
        "id": "Gew_vpjUgwkA",
        "outputId": "6d6b7de8-6a33-4b62-f2ff-34308a170d96"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/8.64k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "844ae9ea9da343df9386921d66d2a10e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e896db0f7f4a43ddbb4a6da4463c8999"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: \n",
            " The task of Question Answering can answer a question given a context and sometimes without context Example: QA chatbots or search engine models like ChatGPT and BingAI to answer general closed domain questionsTokenization is the process of breaking text into individual terms or tokens. Can be as simple as splitting by space, or as complex as recognizing words in a sentence.   Another stage of preprocessing in NLP is stopword and punctuation removal. Stopwords are commonly used words (such as 'the', 'is', 'at') that are filtered out before processing since they add noise without informative content this helps with focused analysis and faster processing Caution is advised as some stopwords can change the meaning of a sentence (e.g., 'not'). Punctuation marks are often removed during text preprocessing because: They can be irrelevant for understanding the meaning of texts, especially in models focusing on individual words. However, in certain contexts like sentiment analysis, exclamation points or question marks can carry sentiment and should be preserved.\n",
            "\n",
            "\n",
            "Question: \n",
            " What is question answering ?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Reference Answer: ': 'The task of Question Answering can answer a question given a context and sometimes without context Example: QA chatbots or search engine models like ChatGPT and BingAI to answer general closed domain questions',\n",
              " 'Predicted Answer: ': 'The task of Question Answering can answer a question given a context and sometimes without context',\n",
              " 'BLEU Score: ': {'google_bleu': 0.4461538461538462}}"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part from below might be redundant if model is trained on the top with t5-base"
      ],
      "metadata": {
        "id": "Ilh4wohnfG_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_name = \"t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "model_max_length = 300\n",
        "\n",
        "# Generate text based on a prompt\n",
        "prompt = f\"{query} {retrieved_response}\"\n",
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=model_max_length, truncation=True)\n",
        "\n",
        "# Generate text with custom parameters\n",
        "output = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_length=120,\n",
        "    num_return_sequences=3,\n",
        "    temperature=0.2,\n",
        "    top_k=100,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=2.0,\n",
        "    length_penalty=0.5,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "tiUZdenCsMUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3vCQmA_NDZu",
        "outputId": "8b16ef84-5d06-42c0-993f-d290e4fa2a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine Learning is important because there is too much data in the world for humans to process, When computational resources are minimal and When model interpretation is required Machine learning is branch of Computer Science It focuses on the use of data and algorithms to imitate the way humans learn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation with pretrained GPT2 model"
      ],
      "metadata": {
        "id": "8v1J2I2uap5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "# Generate text based on a prompt\n",
        "prompt = f\"{query} {retrieved_response}\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text with custom parameters\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=112,             # Maximum length of the generated text\n",
        "    num_return_sequences=3,    # Number of independent sequences to generate\n",
        "    temperature=0.2,           #(0 to 1) Controls the randomness of the generated text the higher the number the more creative the answer\n",
        "    top_k=100,                 #(10 to 100) Controls the diversity of the generated text the greater the number the more tokens will be considered to generate the response\n",
        "    top_p=0.95,                #(0.1 to 0.95) Controls the nucleus sampling for the generated text the higher the number the greater the probabilty of selecting tokens that relate to a different probability distribution.\n",
        "    repetition_penalty=2.0,    #(1.0 to 2.0 value must be float) Penalizes repeated sequences in the generated text the greater the number the more restricted the model in repeating the same sequence\n",
        "    length_penalty=0.5,        #(0.5 to 2.0) Controls the trade-off between length and quality the greater the value the longer the genrated answer which will inturn compromise the quality of the text\n",
        "    num_beams=3                #(1 to 10) Number of beams for beam search decoding (set to 1 for greedy decoding) the greater the value the higher the diversity in the answer.\n",
        ")\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "6D84DaY4sOBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4574aa4-a3a1-4d88-c9a2-9718eea512b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0flzvCqLZlyj",
        "outputId": "b5c01c9a-9c09-4929-b90e-b98085988b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do we need machine learning? Machine Learning is an important field of data science because there is too much data in the world for humans to process and Classical Machine Learning is dependent on human Intervention which is a sub-field of AI that uses algorithms trained on data to produce adaptable models to perform tasks Machine learning is used when the task is simple and structured enough for Machine Learning models, When computational resources are minimal and When model interpretation is required Machine learning is branch of Computer Science It focuses on the use of data and algorithms to imitate the way humans learn.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing with Langchain GPT-3.5\n",
        "\n",
        "NOTE: This requires an API key which will be available for 3 months starting from 14th Feb 2024"
      ],
      "metadata": {
        "id": "VvFbYvngdJ5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "O4J1aL_Bc28u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"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\""
      ],
      "metadata": {
        "id": "4DiIQlEGfMmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "xAG2nc5yerR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q1p_dYRgzwZ",
        "outputId": "64b55437-92f8-4f8e-af2c-3b9267663abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=\"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\")"
      ],
      "metadata": {
        "id": "YrY7juwcg_Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "yzHqGn8uh8O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_nuWOlGwoASd",
        "outputId": "86666bc7-2634-45fe-c8a2-123c922395a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why do we need machine learning ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "chain.invoke({\"context\":all_data,\"question\":query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exii7ZbCiCqD",
        "outputId": "f936fd8f-9cfe-49fa-de66-b1abd9c2afb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='We need machine learning because there is too much data in the world for humans to process, and machine learning can help process and analyze this data. Additionally, machine learning models can be adaptable and perform tasks without human intervention. Machine learning is also useful when the task is simple and structured enough for machine learning models, when computational resources are minimal, and when model interpretation is required.')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b6260d48908f49d18ae04091e2f100ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7100d4f588c142f2ad2962990f014d37",
              "IPY_MODEL_3f6490167e7f4417be46d580b8dbe544",
              "IPY_MODEL_d60a4b8aa7b44b4a81e52e303b449f89"
            ],
            "layout": "IPY_MODEL_9ea11a7c5b3a4b0b9a7274596e5443be"
          }
        },
        "7100d4f588c142f2ad2962990f014d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_415983219610460fae40733a4ed333b4",
            "placeholder": "​",
            "style": "IPY_MODEL_dd8f8ebd73904b03bd4daef9cc1cc540",
            "value": "artifact.metadata: 100%"
          }
        },
        "3f6490167e7f4417be46d580b8dbe544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_075705dddcc0494db559efedbc037761",
            "max": 1633,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d26988f2e49b4045aca8bf981f9b5fe6",
            "value": 1633
          }
        },
        "d60a4b8aa7b44b4a81e52e303b449f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_704b56a89e524829b649d72688d6e4e5",
            "placeholder": "​",
            "style": "IPY_MODEL_93bc3fbb8dde4a94bda3efc9ac0b86da",
            "value": " 1.63k/1.63k [00:00&lt;00:00, 54.0kB/s]"
          }
        },
        "9ea11a7c5b3a4b0b9a7274596e5443be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "415983219610460fae40733a4ed333b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd8f8ebd73904b03bd4daef9cc1cc540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "075705dddcc0494db559efedbc037761": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d26988f2e49b4045aca8bf981f9b5fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "704b56a89e524829b649d72688d6e4e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93bc3fbb8dde4a94bda3efc9ac0b86da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "844ae9ea9da343df9386921d66d2a10e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a692247727b468d917b0f2d4a05741f",
              "IPY_MODEL_24970e00b0ec418a9c0cff1e123842b6",
              "IPY_MODEL_612537e0d11e452f970a6aabd29731d8"
            ],
            "layout": "IPY_MODEL_6ee740c4417e4230bdb9c19c2e39a2fd"
          }
        },
        "7a692247727b468d917b0f2d4a05741f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec38746bdeb84f53a974c28af2025513",
            "placeholder": "​",
            "style": "IPY_MODEL_cd88c93e099c43afa121842dc6ae0e80",
            "value": "Downloading builder script: 100%"
          }
        },
        "24970e00b0ec418a9c0cff1e123842b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a7984d5b8944d4f9e0568a5dc622ced",
            "max": 8645,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c698c10e5d242ed8d3c7216c2141d47",
            "value": 8645
          }
        },
        "612537e0d11e452f970a6aabd29731d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_752f8b7c5ae3418da491e24ad820766e",
            "placeholder": "​",
            "style": "IPY_MODEL_bff6b706fb7a47d7b86b243e8b59a85a",
            "value": " 8.64k/8.64k [00:00&lt;00:00, 9.52kB/s]"
          }
        },
        "6ee740c4417e4230bdb9c19c2e39a2fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec38746bdeb84f53a974c28af2025513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd88c93e099c43afa121842dc6ae0e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a7984d5b8944d4f9e0568a5dc622ced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c698c10e5d242ed8d3c7216c2141d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "752f8b7c5ae3418da491e24ad820766e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bff6b706fb7a47d7b86b243e8b59a85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e896db0f7f4a43ddbb4a6da4463c8999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12a071c3837a4ce7a97acf61e4bc723e",
              "IPY_MODEL_6acd8862ccea44af9ad4ee48e1cf1a03",
              "IPY_MODEL_a170441616734c10b941cf4338275c4e"
            ],
            "layout": "IPY_MODEL_88fbb3635e1040ce88ea5da9aaa83ce1"
          }
        },
        "12a071c3837a4ce7a97acf61e4bc723e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_479123f775cf4917b88aa0c87550ee79",
            "placeholder": "​",
            "style": "IPY_MODEL_f6e05dfb4bb24f459709636f45a3bd7b",
            "value": "Downloading extra modules: 100%"
          }
        },
        "6acd8862ccea44af9ad4ee48e1cf1a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42d78426a40d4568b6960badd245d85b",
            "max": 3344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_562f7fd0d7fc40fba175b797447142fc",
            "value": 3344
          }
        },
        "a170441616734c10b941cf4338275c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d7f07882a6547d390ba1787b7a85647",
            "placeholder": "​",
            "style": "IPY_MODEL_51acb390a0ff45ee982d1b331fb9b918",
            "value": " 3.34k/3.34k [00:00&lt;00:00, 9.43kB/s]"
          }
        },
        "88fbb3635e1040ce88ea5da9aaa83ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "479123f775cf4917b88aa0c87550ee79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6e05dfb4bb24f459709636f45a3bd7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42d78426a40d4568b6960badd245d85b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "562f7fd0d7fc40fba175b797447142fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d7f07882a6547d390ba1787b7a85647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51acb390a0ff45ee982d1b331fb9b918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}