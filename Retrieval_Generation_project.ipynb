{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afsarahannan/NLP_RAG_Project-/blob/main/Retrieval_Generation_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTwpDNBnorUT"
      },
      "source": [
        "# RAG Project: ColBert and GPT3.5\n",
        "\n",
        "If you're working in Google Colab, we recommend selecting \"T4 GPU\" as your hardware accelerator in the runtime settings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl_YBBPTo5AZ",
        "outputId": "6d4e2c9c-2eb4-485f-868e-a64706876c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git\n",
        "import sys; sys.path.insert(0, 'ColBERT/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmBi2UT5pxb3"
      },
      "outputs": [],
      "source": [
        "try: # When on google Colab, let's install all dependencies with pip.\n",
        "    import google.colab\n",
        "    !pip install -U pip\n",
        "    !pip install -e ColBERT/['faiss-gpu','torch']\n",
        "except Exception:\n",
        "  import sys; sys.path.insert(0, 'ColBERT/')\n",
        "  try:\n",
        "    from colbert import Indexer, Searcher\n",
        "  except Exception:\n",
        "    print(\"You're running outside Colab, please make sure you install ColBERT in conda. Conda is recommended.\")\n",
        "    assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "N0jxbVar4kln"
      },
      "outputs": [],
      "source": [
        "import colbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xQg9A-dtp1nB"
      },
      "outputs": [],
      "source": [
        "from colbert import Indexer, Searcher\n",
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert.data import Queries, Collection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the csv dataset here\n",
        "import pandas as pd\n",
        "queries = pd.read_csv(\"/content/queries.csv\", header = None ,sep='\\t')\n",
        "answers = pd.read_csv(\"/content/answers.csv\", header = None ,sep='\\t')"
      ],
      "metadata": {
        "id": "grY5rJ27wVs9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions =[x for x in queries[0]]\n",
        "answer_index = [x for x in queries[1]]\n",
        "answer_index_integers = [[int(num) for num in sub_string.split(',')] for sub_string in answer_index]\n",
        "answer_text = [x for x in answers[0]]\n",
        "print(f\"There are {len(questions)} questions and {len(answer_text)} answers in this notebook.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpkG4eRoyYmm",
        "outputId": "54068818-d713-4db4-cf13-aac3db59cc7d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 72 questions and 83 answers in this notebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJdAAbDu7PZ"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "\n",
        "Below, the `Indexer` take a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from this index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vKAdVN5MvDKD"
      },
      "outputs": [],
      "source": [
        "nbits = 2   # encode each dimension with 2 bits\n",
        "doc_maxlen = 300 # truncate passages at 300 tokens\n",
        "# max_id = 10000\n",
        "\n",
        "index_name = f'ML_Edge.{nbits}bits'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_name"
      ],
      "metadata": {
        "id": "OOvGGR4U6Mld",
        "outputId": "a6b11387-9d70-4bad-e695-cb3d3560bcda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ML_Edge.2bits'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orKfQRmQv46u"
      },
      "source": [
        "Now run the `Indexer` on the collection subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRiOnzxtwI0j"
      },
      "outputs": [],
      "source": [
        "checkpoint = 'colbert-ir/colbertv2.0'\n",
        "\n",
        "with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use\n",
        "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n",
        "                                                                                # Consider larger numbers for small datasets.\n",
        "\n",
        "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
        "    indexer.index(name=index_name, collection=answer_text, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY6_D523yBFB"
      },
      "source": [
        "## Search\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3x_FnVnyB0n"
      },
      "outputs": [],
      "source": [
        "# Create the searcher\n",
        "with Run().context(RunConfig(experiment='notebook')):\n",
        "  searcher = Searcher(index=index_name, collection=answer_text, )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#These are the type of sample questions we can ask the retriever\n",
        "questions"
      ],
      "metadata": {
        "id": "tTmJxKWthVoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JYA0N22yIeS"
      },
      "outputs": [],
      "source": [
        "query = 'Why do we need machine learning ?' # try with an in-range query or supply your own\n",
        "print(f\"Question: {query}\")\n",
        "\n",
        "# Find the top-3 passages for this query\n",
        "results = searcher.search(query, k=3)\n",
        "\n",
        "# Print out the top-k retrieved passages\n",
        "for passage_id, passage_rank, passage_score in zip(*results):\n",
        "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []\n",
        "for passage_id, passage_rank, passage_score in zip(*results):\n",
        "  data = searcher.collection[passage_id]\n",
        "  all_data.append(data)\n",
        "\n",
        "\n",
        "retrieved_response = ''.join(all_data)\n",
        "retrieved_response"
      ],
      "metadata": {
        "id": "HcTmpDO0kLjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data"
      ],
      "metadata": {
        "id": "nhOn5wElsoAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_response"
      ],
      "metadata": {
        "id": "dWtedV6Vssev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "mK6ouJpM7X2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation with pretrained t5-base model"
      ],
      "metadata": {
        "id": "puoVf0KJaeh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the transformers library\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "7saVRpLVaDO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_name = \"t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "model_max_length = 300\n",
        "\n",
        "# Generate text based on a prompt\n",
        "prompt = f\"{query} {retrieved_response}\"\n",
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=model_max_length, truncation=True)\n",
        "\n",
        "# Generate text with custom parameters\n",
        "output = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_length=120,\n",
        "    num_return_sequences=3,\n",
        "    temperature=0.2,\n",
        "    top_k=100,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=2.0,\n",
        "    length_penalty=0.5,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "tiUZdenCsMUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3vCQmA_NDZu",
        "outputId": "8b16ef84-5d06-42c0-993f-d290e4fa2a91"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine Learning is important because there is too much data in the world for humans to process, When computational resources are minimal and When model interpretation is required Machine learning is branch of Computer Science It focuses on the use of data and algorithms to imitate the way humans learn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation with pretrained GPT2 model"
      ],
      "metadata": {
        "id": "8v1J2I2uap5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "# Generate text based on a prompt\n",
        "prompt = f\"{query} {retrieved_response}\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text with custom parameters\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=112,             # Maximum length of the generated text\n",
        "    num_return_sequences=3,    # Number of independent sequences to generate\n",
        "    temperature=0.2,           #(0 to 1) Controls the randomness of the generated text the higher the number the more creative the answer\n",
        "    top_k=100,                 #(10 to 100) Controls the diversity of the generated text the greater the number the more tokens will be considered to generate the response\n",
        "    top_p=0.95,                #(0.1 to 0.95) Controls the nucleus sampling for the generated text the higher the number the greater the probabilty of selecting tokens that relate to a different probability distribution.\n",
        "    repetition_penalty=2.0,    #(1.0 to 2.0 value must be float) Penalizes repeated sequences in the generated text the greater the number the more restricted the model in repeating the same sequence\n",
        "    length_penalty=0.5,        #(0.5 to 2.0) Controls the trade-off between length and quality the greater the value the longer the genrated answer which will inturn compromise the quality of the text\n",
        "    num_beams=3                #(1 to 10) Number of beams for beam search decoding (set to 1 for greedy decoding) the greater the value the higher the diversity in the answer.\n",
        ")\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "6D84DaY4sOBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4574aa4-a3a1-4d88-c9a2-9718eea512b7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0flzvCqLZlyj",
        "outputId": "b5c01c9a-9c09-4929-b90e-b98085988b36"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do we need machine learning? Machine Learning is an important field of data science because there is too much data in the world for humans to process and Classical Machine Learning is dependent on human Intervention which is a sub-field of AI that uses algorithms trained on data to produce adaptable models to perform tasks Machine learning is used when the task is simple and structured enough for Machine Learning models, When computational resources are minimal and When model interpretation is required Machine learning is branch of Computer Science It focuses on the use of data and algorithms to imitate the way humans learn.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing with Langchain GPT-3.5\n",
        "\n",
        "NOTE: This requires an API key which will be available for 3 months starting from 14th Feb 2024"
      ],
      "metadata": {
        "id": "VvFbYvngdJ5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "O4J1aL_Bc28u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"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\""
      ],
      "metadata": {
        "id": "4DiIQlEGfMmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "xAG2nc5yerR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q1p_dYRgzwZ",
        "outputId": "64b55437-92f8-4f8e-af2c-3b9267663abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=\"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\")"
      ],
      "metadata": {
        "id": "YrY7juwcg_Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "yzHqGn8uh8O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_nuWOlGwoASd",
        "outputId": "86666bc7-2634-45fe-c8a2-123c922395a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why do we need machine learning ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "chain.invoke({\"context\":all_data,\"question\":query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exii7ZbCiCqD",
        "outputId": "f936fd8f-9cfe-49fa-de66-b1abd9c2afb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='We need machine learning because there is too much data in the world for humans to process, and machine learning can help process and analyze this data. Additionally, machine learning models can be adaptable and perform tasks without human intervention. Machine learning is also useful when the task is simple and structured enough for machine learning models, when computational resources are minimal, and when model interpretation is required.')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}