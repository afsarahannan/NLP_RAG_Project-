{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afsarahannan/NLP_RAG_Project-/blob/main/Retrieval_Generation_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTwpDNBnorUT"
      },
      "source": [
        "# ColBERTv2: Indexing & Search Notebook\n",
        "\n",
        "If you're working in Google Colab, we recommend selecting \"GPU\" as your hardware accelerator in the runtime settings.\n",
        "\n",
        "First, we'll import the relevant classes. Note that `Indexer` and `Searcher` are the key actors here. Next, we'll download the necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl_YBBPTo5AZ",
        "outputId": "45cea125-69e9-464b-89b6-528c52c5fecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: cannot change to 'ColBERT/': No such file or directory\n",
            "Cloning into 'ColBERT'...\n",
            "remote: Enumerating objects: 2634, done.\u001b[K\n",
            "remote: Counting objects: 100% (1137/1137), done.\u001b[K\n",
            "remote: Compressing objects: 100% (336/336), done.\u001b[K\n",
            "remote: Total 2634 (delta 891), reused 847 (delta 801), pack-reused 1497\u001b[K\n",
            "Receiving objects: 100% (2634/2634), 2.03 MiB | 10.91 MiB/s, done.\n",
            "Resolving deltas: 100% (1650/1650), done.\n"
          ]
        }
      ],
      "source": [
        "!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git\n",
        "import sys; sys.path.insert(0, 'ColBERT/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmBi2UT5pxb3"
      },
      "outputs": [],
      "source": [
        "try: # When on google Colab, let's install all dependencies with pip.\n",
        "    import google.colab\n",
        "    !pip install -U pip\n",
        "    !pip install -e ColBERT/['faiss-gpu','torch']\n",
        "except Exception:\n",
        "  import sys; sys.path.insert(0, 'ColBERT/')\n",
        "  try:\n",
        "    from colbert import Indexer, Searcher\n",
        "  except Exception:\n",
        "    print(\"You're running outside Colab, please make sure you install ColBERT in conda. Conda is recommended.\")\n",
        "    assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0jxbVar4kln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de078aa-9a63-41bf-8aa8-f9e321ae1152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ],
      "source": [
        "import colbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQg9A-dtp1nB"
      },
      "outputs": [],
      "source": [
        "from colbert import Indexer, Searcher\n",
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert.data import Queries, Collection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the csv dataset here\n",
        "import pandas as pd\n",
        "queries = pd.read_csv(\"/content/queries.csv\", header = None ,sep='\\t')\n",
        "answers = pd.read_csv(\"/content/answers.csv\", header = None ,sep='\\t')"
      ],
      "metadata": {
        "id": "grY5rJ27wVs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions =[x for x in queries[0]]\n",
        "answer_index = [x for x in queries[1]]\n",
        "answer_index_integers = [[int(num) for num in sub_string.split(',')] for sub_string in answer_index]\n",
        "answer_text = [x for x in answers[0]]\n",
        "print(f\"There are {len(questions)} questions and {len(answer_text)} answers in this notebook.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpkG4eRoyYmm",
        "outputId": "fafa38bd-1784-4af2-ead9-1802cc458c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 48 questions and 59 answers in this notebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJdAAbDu7PZ"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "\n",
        "Below, the `Indexer` take a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from this index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKAdVN5MvDKD"
      },
      "outputs": [],
      "source": [
        "nbits = 2   # encode each dimension with 2 bits\n",
        "doc_maxlen = 300 # truncate passages at 300 tokens\n",
        "# max_id = 10000\n",
        "\n",
        "index_name = f'ML_Edge.{nbits}bits'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_name"
      ],
      "metadata": {
        "id": "OOvGGR4U6Mld",
        "outputId": "8bff350e-f714-41c6-e005-26e1d770b36c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ML_Edge.2bits'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orKfQRmQv46u"
      },
      "source": [
        "Now run the `Indexer` on the collection subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRiOnzxtwI0j"
      },
      "outputs": [],
      "source": [
        "checkpoint = 'colbert-ir/colbertv2.0'\n",
        "\n",
        "with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use\n",
        "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n",
        "                                                                                # Consider larger numbers for small datasets.\n",
        "\n",
        "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
        "    indexer.index(name=index_name, collection=answer_text, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY6_D523yBFB"
      },
      "source": [
        "## Search\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3x_FnVnyB0n"
      },
      "outputs": [],
      "source": [
        "# Create the searcher\n",
        "with Run().context(RunConfig(experiment='notebook')):\n",
        "    searcher = Searcher(index=index_name, collection=answer_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#These are the type of questions we can ask the retriever\n",
        "questions"
      ],
      "metadata": {
        "id": "tTmJxKWthVoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JYA0N22yIeS",
        "outputId": "c4b0d588-1d45-470f-b478-2170795cb05c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Why do we need machine learning ?\n",
            "\n",
            "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
            "#> Input: . Why do we need machine learning ?, \t\t True, \t\t None\n",
            "#> Output IDs: torch.Size([32]), tensor([ 101,    1, 2339, 2079, 2057, 2342, 3698, 4083, 1029,  102,  103,  103,\n",
            "         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n",
            "         103,  103,  103,  103,  103,  103,  103,  103])\n",
            "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "\t [1] \t\t 25.7 \t\t Machine Learning is an important field of data science because there is too much data in the world for humans to process and Classical Machine Learning is dependent on human Intervention which is a sub-field of AI that uses algorithms trained on data to produce adaptable models to perform tasks  \n",
            "\t [2] \t\t 25.1 \t\t Machine learning is used when the task is simple and structured enough for Machine Learning models, When computational resources are minimal and When model interpretation is required\n",
            "\t [3] \t\t 24.3 \t\t Machine learning is branch of Computer Science It focuses on the use of data and algorithms to imitate the way humans learn \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
          ]
        }
      ],
      "source": [
        "query = 'Why do we need machine learning ?' # try with an in-range query or supply your own\n",
        "print(f\"Question: {query}\")\n",
        "\n",
        "# Find the top-3 passages for this query\n",
        "results = searcher.search(query, k=3)\n",
        "\n",
        "# Print out the top-k retrieved passages\n",
        "for passage_id, passage_rank, passage_score in zip(*results):\n",
        "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []\n",
        "for passage_id, passage_rank, passage_score in zip(*results):\n",
        "  data = searcher.collection[passage_id]\n",
        "  all_data.append(data)\n",
        "\n",
        "\n",
        "retrieved_response = ''.join(all_data)\n",
        "retrieved_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "HcTmpDO0kLjO",
        "outputId": "e3300c38-28d8-4701-9474-1028a171f9cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Machine Learning is an important field of data science because there is too much data in the world for humans to process and Classical Machine Learning is dependent on human Intervention which is a sub-field of AI that uses algorithms trained on data to produce adaptable models to perform tasks  Machine learning is used when the task is simple and structured enough for Machine Learning models, When computational resources are minimal and When model interpretation is requiredMachine learning is branch of Computer Science It focuses on the use of data and algorithms to imitate the way humans learn '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "mK6ouJpM7X2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the transformers library if you haven't already\n",
        "!pip install transformers\n",
        "\n",
        "# Import necessary libraries\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "# Import necessary libraries\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "3DlKp_Y0y-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate query and response\n",
        "input_text = f\"{query} {retrieved_response}\"\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "model_name = \"t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize and generate response\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "generated_ids = model.generate(input_ids, max_length=150, num_return_sequences=1, num_beams=4, early_stopping=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CVz6iAaG7iD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode and print response\n",
        "generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated response:\", generated_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNgoZm6dNsZr",
        "outputId": "5be26cc4-6579-44f5-dddd-528c310d918b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response: Machine Learning is a branch of Computer Science Machine Learning is important because there is too much data in the world for humans to process, When computational resources are minimal and When model interpretation is required Machine Learning is used when the task is simple and structured enough for Machine Learning models, When computational resources are minimal and When computational resources are minimal and When model interpretation is requiredMachine learning is branch of Computer Science It focuses on the use of data and algorithms to imitate the way humans learn. Machine Learning is branch of Computer Science It focuses on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize input and generate response\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "attention_mask = input_ids.clone().detach()\n",
        "attention_mask.fill_(1)  # Setting all tokens to attention\n",
        "generated_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=150, num_return_sequences=1, num_beams=4, early_stopping=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnB9EWymQFnh",
        "outputId": "0234269c-5ae6-4bbd-8fd5-88bb0c693fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode and print response\n",
        "generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated response:\", generated_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdrQS9qGQGlX",
        "outputId": "7115af8d-52e5-46b1-cfd4-1f2625e42a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response: Why do we need machine learning? Machine Learning is an important field of data science because there is too much data in the world for humans to process and Classical Machine Learning is dependent on human Intervention which is a sub-field of AI that uses algorithms trained on data to produce adaptable models to perform tasks  Machine learning is used when the task is simple and structured enough for Machine Learning models, When computational resources are minimal and When model interpretation is requiredMachine learning is branch of Computer Science It focuses on the use of data and algorithms to imitate the way humans learn  Machine Learning is a branch of Computer Science It focuses on the use of data and algorithms to imitate the way humans learn Machine Learning is a branch of Computer Science It focuses on the use of data\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}