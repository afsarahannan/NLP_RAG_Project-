introduction to machine learning ,Machine learning is branch of Computer Science It focuses on the use of data and algorithms to imitate the way humans learn Machine Learning is an important field of data science There is too much data in the world for humans to process Classical Machine Learning is dependent on human Intervention sub-field of AI that uses algorithms trained on data to produce adaptable models to perform tasks a computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T as measured by P improves with experience E Machine Learning is the field of study that gives the computer the ability to learn without being explicitly programmed   
deep learning ,"deep learning can work with or without human intervention Human experts could determine the set of features to learn but not required Can do supervised and unsupervised learning subset of machine learning that uses layers of Neural Networks to do the most complex ML tasks Subset of ML - loosely mimics structure/function of human brain Unlike traditional ML, does not require manual feature extraction Keeps getting better with more data Neural Networks replace the mathematical models Deep learning requires large volume of high-dimensional data Deep Learning can learn high-dimensional, unstructured data
(images, texts, etc.) Deep learning excel in complex tasks requiring hierarchical feature learning When to avoid Deep Learning When task is simple and structured enough for Machine Learning models When computational resources are minimal When model interpretation is required Places Deep Learning is being used today Image Recognition: Automating the process of identifying and detecting objects in images and videos Natural Language Processing: Understanding and generating human language, enabling applications such as chatbots and translation services. Places Deep Learning is being used today Healthcare: Assisting in diagnosis, personalized medicine, and drug discovery. Autonomous Vehicles: Enabling self-driving cars to navigate and understand their environment. Finance: Fraud detection, credit scoring, and algorithmic trading.           "
artificial intelligence ,ai is an umbrella term for software that mimics human cognition to perform complex tasks Speech Recognition A lot of programs allow users to speak instead of typing This increases a program’s accessibility to the differently abled Also allows for a more natural interaction with computers Examples Google Assistant Speech-to-Text keyboards Customer Service The advent of ChatGPT and other chatbots have led to creation of customer service chatbots Computer Vision Everytime you take a picture AI is used to determine the best way to tune color exposure and lighting When the picture is taken AI is used to tag faces in the picture for various reasons Recommendation Engines Using past customer data AI models can help recommend content that customer likes to consume TikTok Youtube Instagram Facebook etc perfected the craft Advertisement platforms are built with the promise of connecting the right ads to the right user Fraud Detection Banks and other financial institutions use AI to spot suspicious transaction This is to protect their customers from malicious intent Email services also detect and delete fraudulent emails from entering your inbox      
how machines learn ,"Machine learning algorithms are used to either make a prediction or classify a given data input. The data may or may not be labeled A function is responsible to evaluate the prediction of a model with it’s true label Models are adjusted to reduce discrepancy between a known example and the model estimate Types of ML Learning supervised learning unsupervised learning reinforcement learning Supervised Learning This is the most popular method to train algorithms A labeled dataset is used to train algorithms The algorithm learns patterns from labeled data during training and predict outcomes for new, unseen data Can be used to learn classification or regression Trained models aim to generalize, by avoiding overfitting/underfitting Generalization: Model should generalize by learning the underlying patterns in the data, rather than memorizing the data exactly Overfitting: Occurs when a model fits the training data too closely and can only train well but test poorly. Model has memorized data Underfitting: Occurs when the model is too simple and has not captured the underlying pattern. Model did not learn Example of models that are used during supervised learning Neural Networks Naive Bayes Random Forest Gradient Boosting Unsupervised Learning An unlabeled dataset is used to train algorithms Used often to uncover hidden patterns or data grouping without human intervention Ideal for Exploratory data analysis Image and Pattern Recognition Example of models that are used during unsupervised learning Neural Networks K-means clustering Principal Component Analysis (PCA) Singular Value Decomposition (SVD) Machine Learning Use DATA and ANSWERS to learn the underlying set of RULES by fine-tuning long list of rules Getting insights from large amounts of data      "
classification of machine learning system,Useful to classify ML systems based on the following criteria if it requires human supervision then Supervised Learning Semisupervised Learning if it does not require human supervision then Unsupervised Learning Reinforcement Learning if the system builds predictive models Model-based Learning Instance-based Learning if it can learn incrementally on the fly online learning or batch learning e.g. Spam filter may learn on the fly with a deep neural network – online model-based supervised learning system 
supervised learning ,Training data fed to algorithm includes the desired answers/solutions (labels) Example algorithms Linear Regression Logistic Regression SVM Decision Tree Neural Network 
unsupervised learning ,Training data is unlabeled System learns without direct human supervision Widely used in Clustering Anomaly detection Association mining Data preprocessing Example algorithms K-means PCA SVD ICA 
semi-supervised learning,Partially labeled data Unsupervised learning used to cluster similar data together Human input taken to label the clusters  
reinforcement-learning ,The learning system (agent) can Observe the environment Select and perform an action Get rewards/penalties as a result Learns what the best policy should be Policy defines what actions should be chosen in a certain situation Very effective in controlled environments (such as a game of chess) With the progress in deep learning increasingly used in more complex tasks (such as driving the mars rover).
batch learning ,Batch Learning Not capable of learning after deployment Must be retrained from scratch computationally expensive
online learning ,Online Learning Can continue to learn after deployment Can take advantage of parallel computing – no down time Preferred choice in production
problems with machine learning ,Insufficient quantity Non-representative data Poor-quality data Overfitting data Underfitting data Most common problem in ML do not overgeneralize 
Avoid overfitting data ,"Constrain model to keep it simple – reduce risk of overfitting Hyperparameters – control level of regularization Get more training data, and reduce noise in it "
Model Evaluation ,"Test it on new data – data not seen by the model ever before Keep 80% for training, set 20% for testing NEVER go below 10% test data better model is better than better “accuracy” in order to regularize Keep a portion of training data held out for validation Alternatively, use cross-validation Pick the hyperparameters that work best on validation for your model on the test dataset. A great model is trained with 60% training data, 20% validation data, and 20% testing data an okay model trained with 70% training data, 15% validation data, and 15% testing data a barely acceptable model trained with 80% training data, 10% validation data, and 10% testing data Only way to know for sure which model works best is to evaluate them Make reasonable assumptions about your data to select model. Why do we need to evaluate models To avoid overfitting/underfitting To ensure a model’s prediction are reliable To evaluate how a model may work in real-world scenarios Aid engineers to make decisions about model deployment How do we evaluate models Any evaluation we make has to be an objective one There are several metrics to evaluate a model, for example accuracy precision recall f1-score Overfitting is when model memorizes training data This is the opposite of learning a pattern and generalizing We can identify if our model is overfitted if: There is high performance on training data There is poor performance on test data Underfitting happens when the model is too simple The underlying data patterns have not been captured We can identify if our model has underfitting if Low performance on training data Low performance on test data A perfect fit can be achieved By first constructing good evaluation metrics to give us feedback on model performance Then tuning hyper-parameters till the performance improves across the board Accuracy is calculated by dividing the total correct predictions with the total number of predictions Ideal Usage When class distributions are balanced Precision counts the true positives out of all the items predicted to be positive Ideal Usage When the cost of false positive is too high Example: Email spam detection Recall counts how many of the true positive items were correctly classified Ideal Usage When the missing a positive is too costly The average of precision and recall Ideal Usage When a balance between precision and recall is required, especially when there is class imbalance regression performance metrics. Mean absolute error is The average of the absolute differences between the predicted values and actual values Ideal Usage: When you want to understand the magnitude of error without regard to direction, especially in contexts where large errors aren't more significant than small ones Mean Squared Error (MAE) The average of the squared differences between the predicted values and actual values Ideal Usage: When larger errors are particularly undesirable and should be penalized more. Such as stock market predictions Root Mean Squared Error (RMSE) The square root of MSE, offering error magnitude in the same units as the predicted values Ideal Usage When you want to interpret the error in the original unit and penalize larger errors. Such as predicting the price of houses R-squared (Coefficient of Determination) Represents the proportion of variance for the dependent variable that's explained by independent variables Ideal Usage: When you want to understand the proportion of the dataset's variability captured by the model. Such as how much variance in exam scores are explained by hours studied Adjusted R-squared Modifies R-squared to account for the number of predictors in the model, penalizing excessive use of features. Mean Bias deviation (MBD) The average difference between the predicted and actual values, indicating the direction of the error Ideal Usage: When you're interested in the direction of the error (overestimation vs. underestimation). Helps vary it over or under the limits set                     "
statistical machine learning ,"Statistical Machine Learning emphasizes on the statistical properties of datasets This is most commonly used where predictions are paramount Stock Market forecasting Medical Diagnosis statistical models play the role of the function that is “fitted” onto the
Dataset The model takes in data X and predicts an output Y We evaluate the model with loss functions Lets learn a few of those models today     "
linear regression ,Linear Regression is one of the simplest and most widely used statistical technique It’s goal is to model a relationship between a single dependent variable with one or more multiple independent variable. We use linear regression when the relationship between the independent and dependent variable is believed to be linear. Continuous Output: When predicting values that are continuous (e.g house prices temperatures). Interpretability: When it's important to understand the influence of each feature on the output. Linear regression provides coefficients for each feature which indicate their relative importance. Linear Regression works best when there is a linear relationship between the predictors and the response Regression tasks predict a value based on input data Works best when Residuals are normally distributed Residuals have constant variance Cost Function in Linear Regression Mean Square Error (MSE): Average squared difference between actual and predicted values Adjust model parameters to minimize MSE      
logistic regression ,"Linear Regression is good for predicting continuous value but not much else Logistic Regression predicts the probability of occurrence of an event by fitting data to a logistic curve When to use Logistic Regression Binary Outcome: When the dependent variable is binary (e.g., spam or not spam, churn or not churn) Probabilistic Results: When you need to know the probability of your output. Logistic regression doesn’t just give a binary outcome it gives the probability of that outcome Feature Importance: Similar to linear regression, logistic regression provides coefficients that can help in understanding the influence of features. Cost Function in Logistic Regression Log-Loss: Measure the performance of a classification model whose output probability value is between 0 and 1  "
decision trees ,"Decision Trees split data into subsets This process is repeated recursively Results in a tree-like model of decisions Components of Decision Trees Root Node: Represents the entire dataset, gets divided Decision Node: When a sub-node splits into further sub-nodes Leaf Node: Nodes that contain the decision to be taken The splitting criteria for the decision tree are Impurity: Measures how often a randomly chosen element would be incorrectly classified Entropy: Measures randomness or unpredictability in the dataset Information Gain: The entropy of the original dataset minus the weighted average entropy of the split datasets. Issues in decision tree can include Overfitting: When a model captures noise in the training data and performs poorly on new, unseen data. Complex Trees: Trees that are too deep can capture noise. Pruning: Process of reducing the size of a tree by turning some branch nodes into leaf nodes to reduce complexity. When to use decision trees Non-linear Relationships: Decision trees can capture nonlinear relationships between features and the target variable Interpretability: They are easy to visualize and understand, making them great for deriving insights and rules Categorical Input Features: They handle categorical variables easily Feature Interactions: Decision trees can inherently capture interactions between features    "
random forests ,"Random Forests are an ensemble of decision trees Each tree in a Random Forest is trained on a random subset of data Bagging: A feature of Random Forests that aggregates decision of individual trees and reduces variance. Advantages of using random forest includes Reduction in Overfitting: Diversity among trees reduces chances of overfitting. Feature Importance: Ability to rank features based on their importance in making predictions. Handling Missing Values: Can handle missing data without explicit imputation Generalization: Often generalizes better to new data than individual trees. Disadvantages of random forests Interpretability: Harder to interpret than a single decision tree. Computation: Requires more computational resources. Forest Size: Need to choose the number of trees (more isn't always better). When to use random forests High Accuracy: When performance is a primary concern. Random forests generally yield better accuracy than individual decision trees Feature Importance: Random forests can rank features based on their importance in making accurate predictions. Handling Overfitting: Random Forests, through bagging, tend to reduce the overfitting that can be observed with individual decision trees. Handling Large Data: They can handle datasets with a higher dimensionality and can manage missing values. Non-linear Data: They can capture non-linear feature interactions.     "
Support vector machines ,support vector machine is a supervised ML algorithm which can be used for both classification or regression It performs classification by finding the hyperplane that best divides dataset into classes Hyperplane is a generalized plane in different dimensions. When support vector machines are used for text classification problems when there is a need for margin separation for complex datasets where linear separation is not obvious the drawbacks of support vector machine includes it is inefficient on large datasets it is sensitive to noise and requires fine tuning using parameters such as the kernels support vector machines should be avoided when there is a large dataset when the dataset has a lot of noise when there is no clear margin or separation  
k nearest neighbors ,"K nearest neighbors is a non-parametric, lazy learning algorithm It assumes the similarity between the new data input with the available data Then assigns the new data into the category that is most similar to the available data categories when to use k nearest neighbors When the dataset is relatively small The data has little noise The data has decision boundaries which are very irregular the drawbacks of using k nearest neighbors is that KNN becomes significantly slower as the number of examples grows It is sensitive to irrelevant or redundant features as all features contribute to the similarity. KNN should be avoided when there is a relatively large dataset when the data has a high number of dimension and when the dataset has a lot of noise.        "
gradient boosting algorithms ,Gradient Boosting is a boosting algorithm that combines several weak learners into strong learners It is an ensemble method the Initialization Begins with a simple model It builds a sequence of trees where each new tree corrects the errors of its predecessors these are called residuals. Gradient boosting is used when there is an unbalanced dataset or when the model performance is the primary concern. Drawbacks of such a model is that it can overfit on noisy data requires careful tuning of parameters and Longer training time as trees are built sequentially when to avoid K nearest neighbors when time is a constraint or when the task is too simple and a simple model will suffice.          
Neural Networks ,"Neural Networks are the backbone of Deep Learning These are mathematical model based on human brain structure At a high level these consists of Nodes/Neurons which host a value Connections from one node to another Basic Components of a Neural Network Neurons: Fundamental units of a neural network that receive input and pass the output to the next layer after computation. Weights: Parameters within the network that transform input data within the network's layers. Biases: Additional parameters that enable the model to adjust its output accordingly. Activation Functions: Determine if a neuron should be activated or not, influencing the model's output.   "
